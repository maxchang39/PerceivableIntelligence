{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The goal of this notebook is to flatten the code from CS294-HW3, parameters will be hard coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "\n",
    "# the gym environment name\n",
    "env_name = \"CartPole-v0\"\n",
    "#env_name = \"Breakout-ram-v0\"\n",
    "\n",
    "# number of hidden layers\n",
    "n_layers = 2\n",
    "\n",
    "# random seed\n",
    "seed_init = 0\n",
    "\n",
    "# number of interations\n",
    "n_iter = 100\n",
    "\n",
    "# maximum steps can be taken by the simulation\n",
    "max_path_length = None\n",
    "\n",
    "# dimension of the hidden layer\n",
    "size = 64\n",
    "\n",
    "# default LR\n",
    "learning_rate = 5e-3\n",
    "\n",
    "# number of iterations in one update of critic\n",
    "num_target_updates = 10\n",
    "\n",
    "# number of gradients per iteration \n",
    "num_grad_steps_per_target_update = 10\n",
    "\n",
    "# corresponds to 'render', whether to animate the trajactory or not\n",
    "animate = \"store_true\"\n",
    "\n",
    "# each timestep is a single cycle of the decision sequence\n",
    "min_timesteps_per_batch = 1000\n",
    "\n",
    "# discount over future reward, assume no discount for now\n",
    "gamma = 1.0\n",
    "\n",
    "# Whether the advantage should be normalized or not\n",
    "normalize_advantages = \"store_true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This test works on environment [CartPole-v0, isDiscrete: True, ob_dim: 4, ac_dim:2]\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment and randomness\n",
    "env = gym.make(env_name)\n",
    "seed = seed_init + 10*1\n",
    "\n",
    "# Set random seeds\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "# Maximum length for episodes, 'or' served as a simple None check\n",
    "# max number of steps in environment\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Is this env continuous, or self.discrete?\n",
    "# check if env.action_space is a type of gym.spaces.Discrete\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# printoOut\n",
    "tp = (env_name, \"isDiscrete: \" + str(discrete), \"ob_dim: \"+ str(ob_dim), \"ac_dim:\" + str(ac_dim))\n",
    "print(\"This test works on environment [\" + ', '.join(tp) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, computation_graph_args, sample_trajectory_args, estimate_advantage_args):\n",
    "        super(Agent, self).__init__()\n",
    "        self.ob_dim = computation_graph_args['ob_dim']\n",
    "        self.ac_dim = computation_graph_args['ac_dim']\n",
    "        self.discrete = computation_graph_args['discrete']\n",
    "        self.size = computation_graph_args['size']\n",
    "        self.n_layers = computation_graph_args['n_layers']\n",
    "        self.learning_rate = computation_graph_args['learning_rate']\n",
    "        self.num_target_updates = computation_graph_args['num_target_updates']\n",
    "        self.num_grad_steps_per_target_update = computation_graph_args['num_grad_steps_per_target_update']\n",
    "\n",
    "        self.animate = sample_trajectory_args['animate']\n",
    "        self.max_path_length = sample_trajectory_args['max_path_length']\n",
    "        self.min_timesteps_per_batch = sample_trajectory_args['min_timesteps_per_batch']\n",
    "\n",
    "        self.gamma = estimate_advantage_args['gamma']\n",
    "        self.normalize_advantages = estimate_advantage_args['normalize_advantages']\n",
    "\n",
    "    # Get a runnable session\n",
    "    def init_tf_sess(self):\n",
    "        tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "        tf_config.gpu_options.allow_growth = True # may need if using GPU\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        self.sess.__enter__() # equivalent to `with self.sess:`\n",
    "        tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        \"\"\"\n",
    "            Placeholders for batch observations / actions / advantages in actor critic\n",
    "            loss function.\n",
    "            See Agent.build_computation_graph for notation\n",
    "\n",
    "            returns:\n",
    "                sy_ob_no: placeholder for observations\n",
    "                sy_ac_na: placeholder for actions\n",
    "                sy_adv_n: placeholder for advantages\n",
    "        \"\"\"\n",
    "        sy_ob_no = tf.placeholder(shape=[None, self.ob_dim], name=\"ob\", dtype=tf.float32)\n",
    "        if self.discrete:\n",
    "            sy_ac_na = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32)\n",
    "        else:\n",
    "            sy_ac_na = tf.placeholder(shape=[None, self.ac_dim], name=\"ac\", dtype=tf.float32)\n",
    "\n",
    "        # YOUR HW2 CODE HERE\n",
    "        sy_adv_n = tf.placeholder(shape=[None], name=\"adv\", dtype=tf.float32)\n",
    "        return sy_ob_no, sy_ac_na, sy_adv_n\n",
    "\n",
    "    def policy_forward_pass(self, sy_ob_no):\n",
    "        \"\"\" Constructs the symbolic operation for the policy network outputs,\n",
    "            which are the parameters of the policy distribution p(a|s)\n",
    "\n",
    "            arguments:\n",
    "                sy_ob_no: (batch_size, self.ob_dim)\n",
    "\n",
    "            returns:\n",
    "                the parameters of the policy.\n",
    "\n",
    "                if discrete, the parameters are the logits of a categorical distribution\n",
    "                    over the actions\n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "\n",
    "                if continuous, the parameters are a tuple (mean, log_std) of a Gaussian\n",
    "                    distribution over actions. log_std should just be a trainable\n",
    "                    variable, not a network output.\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "\n",
    "            Hint: use the 'build_mlp' function to output the logits (in the discrete case)\n",
    "                and the mean (in the continuous case).\n",
    "                Pass in self.n_layers for the 'n_layers' argument, and\n",
    "                pass in self.size for the 'size' argument.\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            # YOUR HW2 CODE HERE\n",
    "            sy_logits_na = build_mlp(sy_ob_no, self.ac_dim, \"policy_forward_pass\", self.n_layers, self.size)\n",
    "            return sy_logits_na\n",
    "        else:\n",
    "            # YOUR HW2 CODE HERE\n",
    "            sy_mean = build_mlp(sy_ob_no, self.ac_dim, \"policy_forward_pass\", self.n_layers, self.size)\n",
    "            sy_logstd = tf.get_variable(\"logstd\", shape=[self.ac_dim], trainable=True)\n",
    "            return (sy_mean, sy_logstd)\n",
    "\n",
    "    def sample_action(self, policy_parameters):\n",
    "        \"\"\" Constructs a symbolic operation for stochastically sampling from the policy\n",
    "            distribution\n",
    "\n",
    "            arguments:\n",
    "                policy_parameters\n",
    "                    if discrete: logits of a categorical distribution over actions\n",
    "                        sy_logits_na: (batch_size, self.ac_dim)\n",
    "                    if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                        sy_mean: (batch_size, self.ac_dim)\n",
    "                        sy_logstd: (self.ac_dim,)\n",
    "\n",
    "            returns:\n",
    "                sy_sampled_ac:\n",
    "                    if discrete: (batch_size)\n",
    "                    if continuous: (batch_size, self.ac_dim)\n",
    "\n",
    "            Hint: for the continuous case, use the reparameterization trick:\n",
    "                 The output from a Gaussian distribution with mean 'mu' and std 'sigma' is\n",
    "\n",
    "                      mu + sigma * z,         z ~ N(0, I)\n",
    "\n",
    "                 This reduces the problem to just sampling z. (Hint: use tf.random_normal!)\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            sy_logits_na = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            samples = tf.multinomial(logits=sy_logits_na, num_samples=1) # output of shape [batch_size, num_samples]\n",
    "            sy_sampled_ac = tf.reshape(samples, [-1]) # flatten to be of shape [batch_size]\n",
    "        else:\n",
    "            sy_mean, sy_logstd = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            z = tf.random_normal(tf.shape(sy_mean), mean=0.0, stddev=1.0)\n",
    "            sy_std = tf.exp(sy_logstd)\n",
    "            sy_sampled_ac = sy_mean + sy_std * z\n",
    "            #sy_sampled_ac = sy_mean\n",
    "        return sy_sampled_ac\n",
    "\n",
    "    def get_log_prob(self, policy_parameters, sy_ac_na):\n",
    "        \"\"\" Constructs a symbolic operation for computing the log probability of a set of actions\n",
    "            that were actually taken according to the policy\n",
    "\n",
    "            arguments:\n",
    "                policy_parameters\n",
    "                    if discrete: logits of a categorical distribution over actions\n",
    "                        sy_logits_na: (batch_size, self.ac_dim)\n",
    "                    if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                        sy_mean: (batch_size, self.ac_dim)\n",
    "                        sy_logstd: (self.ac_dim,)\n",
    "\n",
    "                sy_ac_na: (batch_size, self.ac_dim)\n",
    "\n",
    "            returns:\n",
    "                sy_logprob_n: (batch_size)\n",
    "\n",
    "            Hint:\n",
    "                For the discrete case, use the log probability under a categorical distribution.\n",
    "                For the continuous case, use the log probability under a multivariate gaussian.\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            sy_logits_na = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            sy_logprob_n = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels=sy_ac_na, logits=sy_logits_na)\n",
    "        else:\n",
    "            sy_mean, sy_logstd = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            # calculate the z score of the sampled actions under the policy\n",
    "            sy_z = (sy_ac_na - sy_mean) / tf.exp(sy_logstd)\n",
    "            # this maximizes likelihood by pushing z towards 0 (mean of distribution)\n",
    "            sy_logprob_n = -0.5 * tf.reduce_mean(tf.square(sy_z), axis=1)\n",
    "        return sy_logprob_n\n",
    "\n",
    "    def build_computation_graph(self):\n",
    "        \"\"\"\n",
    "            Notes on notation:\n",
    "\n",
    "            Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "            that are computed later in the function\n",
    "\n",
    "            Prefixes and suffixes:\n",
    "            ob - observation\n",
    "            ac - action\n",
    "            _no - this tensor should have shape (batch self.size /n/, observation dim)\n",
    "            _na - this tensor should have shape (batch self.size /n/, action dim)\n",
    "            _n  - this tensor should have shape (batch self.size /n/)\n",
    "\n",
    "            Note: batch self.size /n/ is defined at runtime, and until then, the shape for that axis\n",
    "            is None\n",
    "\n",
    "            ----------------------------------------------------------------------------------\n",
    "            loss: a function of self.sy_logprob_n and self.sy_adv_n that we will differentiate\n",
    "                to get the policy gradient.\n",
    "        \"\"\"\n",
    "        self.sy_ob_no, self.sy_ac_na, self.sy_adv_n = self.define_placeholders()\n",
    "\n",
    "        # The policy takes in an observation and produces a distribution over the action space\n",
    "        self.policy_parameters = self.policy_forward_pass(self.sy_ob_no)\n",
    "\n",
    "        # We can sample actions from this action distribution.\n",
    "        # This will be called in Agent.sample_trajectory() where we generate a rollout.\n",
    "        self.sy_sampled_ac = self.sample_action(self.policy_parameters)\n",
    "\n",
    "        # We can also compute the logprob of the actions that were actually taken by the policy\n",
    "        # This is used in the loss function.\n",
    "        self.sy_logprob_n = self.get_log_prob(self.policy_parameters, self.sy_ac_na)\n",
    "\n",
    "        actor_loss = tf.reduce_sum(-self.sy_logprob_n * self.sy_adv_n)\n",
    "        self.actor_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(actor_loss)\n",
    "\n",
    "        # define the critic\n",
    "        self.critic_prediction = tf.squeeze(build_mlp(\n",
    "                                self.sy_ob_no,\n",
    "                                1,\n",
    "                                \"nn_critic\",\n",
    "                                n_layers=self.n_layers,\n",
    "                                size=self.size))\n",
    "        self.sy_target_n = tf.placeholder(shape=[None], name=\"critic_target\", dtype=tf.float32)\n",
    "        self.critic_loss = tf.losses.mean_squared_error(self.sy_target_n, self.critic_prediction)\n",
    "        self.critic_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "\n",
    "    def sample_trajectories(self, itr, env):\n",
    "        # Collect paths until we have enough timesteps\n",
    "        timesteps_this_batch = 0\n",
    "        paths = []\n",
    "        while True:\n",
    "            animate_this_episode=(len(paths)==0 and (itr % 1 == 0) and self.animate)\n",
    "            path = self.sample_trajectory(env, animate_this_episode)\n",
    "            paths.append(path)\n",
    "            timesteps_this_batch += pathlength(path)\n",
    "            if timesteps_this_batch > self.min_timesteps_per_batch:\n",
    "                break\n",
    "        return paths, timesteps_this_batch\n",
    "\n",
    "    def sample_trajectory(self, env, animate_this_episode):\n",
    "        ob = env.reset()\n",
    "        obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.04)\n",
    "            obs.append(ob)\n",
    "            ac = self.sess.run(self.sy_sampled_ac, feed_dict={self.sy_ob_no: np.expand_dims(ob, axis=0)}) # YOUR HW2 CODE HERE\n",
    "            ac = ac[0]\n",
    "            acs.append(ac)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            # add the observation after taking a step to next_obs\n",
    "            # YOUR CODE HERE\n",
    "            next_obs.append(ob)\n",
    "            rewards.append(rew)\n",
    "            steps += 1\n",
    "            # If the episode ended, the corresponding terminal value is 1\n",
    "            # otherwise, it is 0\n",
    "            # YOUR CODE HERE\n",
    "            if done or steps > self.max_path_length:\n",
    "                terminals.append(1)\n",
    "                break\n",
    "            else:\n",
    "                terminals.append(0)\n",
    "        path = {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "                \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "                \"action\" : np.array(acs, dtype=np.float32),\n",
    "                \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "                \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "        return path\n",
    "\n",
    "    def estimate_advantage(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "        \"\"\"\n",
    "            Estimates the advantage function value for each timestep.\n",
    "\n",
    "            let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "                Agent.sample_trajectories\n",
    "\n",
    "            arguments:\n",
    "                ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "                next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n",
    "                re_n: length: sum_of_path_lengths. Each element in re_n is a scalar containing\n",
    "                    the reward for each timestep\n",
    "                terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n",
    "                    at that timestep of 0 if the episode did not end\n",
    "\n",
    "            returns:\n",
    "                adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                    advantages whose length is the sum of the lengths of the paths\n",
    "        \"\"\"\n",
    "        # First, estimate the Q value as Q(s, a) = r(s, a) + gamma*V(s')\n",
    "        # To get the advantage, subtract the V(s) to get A(s, a) = Q(s, a) - V(s)\n",
    "        # This requires calling the critic twice --- to obtain V(s') when calculating Q(s, a),\n",
    "        # and V(s) when subtracting the baseline\n",
    "        # Note: don't forget to use terminal_n to cut off the V(s') term when computing Q(s, a)\n",
    "        # otherwise the values will grow without bound.\n",
    "        # YOUR CODE HERE\n",
    "        v_s = self.sess.run(self.critic_prediction, feed_dict={self.sy_ob_no: ob_no})\n",
    "        v_s_next = self.sess.run(self.critic_prediction, feed_dict={self.sy_ob_no: next_ob_no})\n",
    "        adv_n = re_n + (1 - terminal_n) * self.gamma * v_s_next - v_s\n",
    "\n",
    "        if self.normalize_advantages:\n",
    "            adv_n = (adv_n - adv_n.mean()) / adv_n.std() # YOUR HW2 CODE_HERE\n",
    "        return adv_n\n",
    "\n",
    "    def update_critic(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "        \"\"\"\n",
    "            Update the parameters of the critic.\n",
    "\n",
    "            let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "                Agent.sample_trajectories\n",
    "            let num_paths be the number of paths sampled from Agent.sample_trajectories\n",
    "\n",
    "            arguments:\n",
    "                ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "                next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n",
    "                re_n: length: sum_of_path_lengths. Each element in re_n is a scalar containing\n",
    "                    the reward for each timestep\n",
    "                terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n",
    "                    at that timestep of 0 if the episode did not end\n",
    "\n",
    "            returns:\n",
    "                nothing\n",
    "        \"\"\"\n",
    "        # Use a bootstrapped target values to update the critic\n",
    "        # Compute the target values r(s, a) + gamma*V(s') by calling the critic to compute V(s')\n",
    "        # In total, take n=self.num_grad_steps_per_target_update*self.num_target_updates gradient update steps\n",
    "        # Every self.num_grad_steps_per_target_update steps, recompute the target values\n",
    "        # by evaluating V(s') on the updated critic\n",
    "        # Note: don't forget to use terminal_n to cut off the V(s') term when computing the target\n",
    "        # otherwise the values will grow without bound.\n",
    "        # YOUR CODE HERE\n",
    "        for _ in range(self.num_target_updates):\n",
    "            v_s_next = self.sess.run(self.critic_prediction, feed_dict={self.sy_ob_no: next_ob_no})\n",
    "            target_values = re_n + (1 - terminal_n) * self.gamma * v_s_next\n",
    "\n",
    "            for _ in range(self.num_grad_steps_per_target_update):\n",
    "                self.sess.run(self.critic_update_op, feed_dict={self.sy_target_n: target_values, self.sy_ob_no: ob_no})\n",
    "\n",
    "    def update_actor(self, ob_no, ac_na, adv_n):\n",
    "        \"\"\"\n",
    "            Update the parameters of the policy.\n",
    "\n",
    "            arguments:\n",
    "                ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "                ac_na: shape: (sum_of_path_lengths).\n",
    "                adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                    advantages whose length is the sum of the lengths of the paths\n",
    "\n",
    "            returns:\n",
    "                nothing\n",
    "\n",
    "        \"\"\"\n",
    "        self.sess.run(self.actor_update_op,\n",
    "            feed_dict={self.sy_ob_no: ob_no, self.sy_ac_na: ac_na, self.sy_adv_n: adv_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================================#\n",
    "# Utilities\n",
    "#============================================================================================#\n",
    "\n",
    "def build_mlp(input_placeholder, output_size, scope, n_layers, size, activation=tf.tanh, output_activation=None):\n",
    "    \"\"\"\n",
    "        Builds a feedforward neural network\n",
    "\n",
    "        arguments:\n",
    "            input_placeholder: placeholder variable for the state (batch_size, input_size)\n",
    "            output_size: size of the output layer\n",
    "            scope: variable scope of the network\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of the hidden layer\n",
    "            activation: activation of the hidden layers\n",
    "            output_activation: activation of the ouput layers\n",
    "\n",
    "        returns:\n",
    "            output placeholder of the network (the result of a forward pass)\n",
    "\n",
    "        Hint: use tf.layers.dense\n",
    "    \"\"\"\n",
    "    inputs = input_placeholder\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for layer in range(n_layers):\n",
    "            inputs = tf.layers.dense(inputs=inputs, units=size, activation=activation)\n",
    "\n",
    "        output_placeholder = tf.layers.dense(inputs=inputs, units=output_size, activation=output_activation)\n",
    "\n",
    "    return output_placeholder\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def setup_logger(logdir, locals_):\n",
    "    # Configure output directory for logging\n",
    "    logz.configure_output_dir(logdir)\n",
    "    # Log experimental parameters\n",
    "    args = inspect.getargspec(train_AC)[0]\n",
    "    params = {k: locals_[k] if k in locals_ else None for k in args}\n",
    "    logz.save_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0713 21:43:59.329718 140735683527552 deprecation.py:323] From <ipython-input-5-e5e140580e84>:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0713 21:43:59.337652 140735683527552 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0713 21:43:59.896758 140735683527552 deprecation.py:323] From <ipython-input-4-001da40f0e4f>:111: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "W0713 21:44:00.218770 140735683527552 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "computation_graph_args = {\n",
    "        'n_layers': n_layers,\n",
    "        'ob_dim': ob_dim,\n",
    "        'ac_dim': ac_dim,\n",
    "        'discrete': discrete,\n",
    "        'size': size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_target_updates': num_target_updates,\n",
    "        'num_grad_steps_per_target_update': num_grad_steps_per_target_update,\n",
    "        }\n",
    "\n",
    "sample_trajectory_args = {\n",
    "    'animate': animate,\n",
    "    'max_path_length': max_path_length,\n",
    "    'min_timesteps_per_batch': min_timesteps_per_batch,\n",
    "}\n",
    "\n",
    "estimate_advantage_args = {\n",
    "    'gamma': gamma,\n",
    "    'normalize_advantages': normalize_advantages,\n",
    "}\n",
    "\n",
    "agent = Agent(computation_graph_args, sample_trajectory_args, estimate_advantage_args) #estimate_return_args\n",
    "\n",
    "# build computation graph\n",
    "agent.build_computation_graph()\n",
    "\n",
    "# tensorflow: config, session, variable initialization\n",
    "agent.init_tf_sess()\n",
    "\n",
    "total_timesteps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n",
      "32.125\n",
      "********** Iteration 1 ************\n",
      "38.703705\n",
      "********** Iteration 2 ************\n",
      "47.95238\n",
      "********** Iteration 3 ************\n",
      "67.933334\n",
      "********** Iteration 4 ************\n",
      "80.38461\n",
      "********** Iteration 5 ************\n",
      "93.09091\n",
      "********** Iteration 6 ************\n",
      "143.125\n",
      "********** Iteration 7 ************\n",
      "177.16667\n",
      "********** Iteration 8 ************\n",
      "180.0\n",
      "********** Iteration 9 ************\n",
      "186.5\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "\n",
    "try:\n",
    "    for itr in range(n_iter):\n",
    "        print(\"********** Iteration %i ************\"%itr)\n",
    "        paths, timesteps_this_batch = agent.sample_trajectories(itr, env)\n",
    "        \n",
    "        total_timesteps += timesteps_this_batch\n",
    "    \n",
    "        # Build arrays for observation, action for the policy gradient update by concatenating\n",
    "        # across paths\n",
    "        ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "        re_n = np.concatenate([path[\"reward\"] for path in paths])\n",
    "        next_ob_no = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "        terminal_n = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "    \n",
    "        # Call tensorflow operations to:\n",
    "        # (1) update the critic, by calling agent.update_critic\n",
    "        # (2) use the updated critic to compute the advantage by, calling agent.estimate_advantage\n",
    "        # (3) use the estimated advantage values to update the actor, by calling agent.update_actor\n",
    "    \n",
    "        # YOUR CODE HERE\n",
    "        agent.update_critic(ob_no, next_ob_no, re_n, terminal_n)\n",
    "        adv_n = agent.estimate_advantage(ob_no, next_ob_no, re_n, terminal_n)\n",
    "        agent.update_actor(ob_no, ac_na, adv_n)\n",
    "        \n",
    "        returns = [path[\"reward\"].sum() for path in paths]\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 20:07:59.790956 140735683527552 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sy_logstd = tf.get_variable(\"logstd\", shape=[3], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'logstd:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(sy_logstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
