{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to describe the design of G-interpreted intelligence.\n",
    "\n",
    "G-interpreted Intelligence is a black-box mathmetical model of intelligence. Intelligence through behavior interpretation and intention inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. G-interpreted Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many subfields of AI tries to solve the problem - how to make a intelligent agent. The objective of this paper is to bridge the gap between engineering approaches from RL and other field, and the search of a logic behind general intelligence. In this paper, we enbrace the natural idea described in reinforcement learning - the assumption that a intelligent system is an autonomous system pursuing optimization over its input space.\n",
    "\n",
    "In this case, we are trying to optimize according to the reward signal.\n",
    "\n",
    "On the other hand, raising concern about the interpretability of such complicated system. One question behind the screen is that, whether we are able to understand all different presentation of intelligence or not? What's the relationship between the observer and the intelligence which observed from another entity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 G function and G value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the objective function from Reinforcement Learning, where $\\theta^*$ is the optimal policy and $E_{\\tau \\sim p_\\theta(\\tau)}$ is the expected cumulative reward of trajectory $\\tau = \\{a_1, o_1,..., a_n, o_n\\}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} E_{\\tau \\sim p_\\theta(\\tau)}[ \\sum_{t}^{\\infty} r(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "In our case, instead of assuming there is always a single special signal which represents the incentive, we believe that an intelligent system can drive itself to the maximization of a goal value which transformed from a sequence of input. \n",
    "\n",
    "Thus, we can construct a function $G(o)$ to maximize. The goal function can be seen as the intention of a intelligent system. The identification of intelligence is equivalent to the search of the intention.\n",
    "\n",
    "**Assumption**\n",
    "\n",
    "**Textual Definition**: An intelligent system is a system which attempts to maximize its goal value\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} E_{\\tau \\sim p_\\theta(\\tau)}[G(o_t)]\n",
    "\\end{equation}\n",
    "\n",
    "Because we don't know about the underlying structure, any structure of agent can serve us welll if it meets the requirement we set up. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_{t} + \\alpha \\frac{d G}{d \\theta} G(o_{t})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 W value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the effort of the agent given time $t$, we can construct a $W$ value as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "W_{\\{s_0,\\pi_\\theta,G\\}}(t) = E\\big(G(o_t)|a_{1}^{\\pi_\\theta},o_1...a_{t}^{\\pi_\\theta},o_{t}\\big).\n",
    "\\end{equation}\n",
    "\n",
    "$s_0$ - the inital state of the environment before the first observation $o_1$\n",
    "\n",
    "$\\pi$ - the observed agent defined by its policy $\\pi$\n",
    "\n",
    "Basically, the $W$ function tells us the expected $G$ value an agent $\\pi$ can achieve from state $s_0$ after time $t$. We use the policy $\\pi$ to identify different agent in order to keep our model independent from the underlying structure of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Identification of agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our definition of agent: \n",
    "\n",
    "Agent $\\prod$ and $\\prod '$ are identical if their policy $\\pi = \\pi '$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Zero-point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hardly discussed question in AI is - how we can tell something is not intelligent. In other word, the zero-point of intelligence is hardly discussed. How can we define its minimum if intelligence is a single-dimensioned metrics.\n",
    "\n",
    "The randomness of agent needs to be discussed when we know about the environment dynamics. But for now,let's assume we have no background knowledge of the environment and the agent at all and use a simple agent following random policy.\n",
    "\n",
    "Therefore, our baseline can be constructed as\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\pi_{R},G\\}}(t) = E\\big(G(o_t) \\sim \\{ a_{1}^{\\pi_{R}},o_1...a_{t}^{\\pi_{R}},o_{t} \\} \\big). \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Definiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of intelligence can now be written in a mathemetical form. Since we only need to find an arbitary goal function. In order to tell if a system is intelligent or not, we just need to find one explanation of the behavior of the agent.\n",
    "\n",
    "**Theory**. For an observation starts at state $s_0$ and lasts for time $T$, a system following policy $\\pi$ is **Perceviably Intelligent** if there exists a Goal function $G$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,G\\}}(\\pi, T) - W_{\\{s_0,G\\}}(R, T) > \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $\\epsilon$ is the acceptance constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 I-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2, we describe the minimum requirement for a system to be intelligent. But the more interesting problem we need to answer is how we can compare two separate systems, or system facing different configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if we follow the same logic from W-value, then it would be clear for us to define the comparable scalar of intelligence repect to the tuple $\\{s_0, G, R\\}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "I_{s_0, G, R}(\\pi) = \\frac {d\\, W_{s_0, G, R}}{d\\,t} \\approx \\frac{1}{T} \\big( W_{s_0,G}(\\pi, T) - W_{s_0,G}(R, T) \\big)\n",
    "\\end{equation}\n",
    "\n",
    "We call this scalar I-value and the ${\\{s_0, G, R\\}}$ as the **Metric**. Apparently, there are some problems with this over-simplified model. We will try to address some of the concerns in later section. The later sections of this paper attempts to generalize this model of intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.1 I-value with normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the optimal policy, we can normalize our I-value using the optimal policy\n",
    "\n",
    "\\begin{equation}\n",
    "I_{s_0, G, R}^{norm}(\\pi) = \\frac{I_{s_0, G, R}(\\pi)}{I_{s_0, G, R}(\\pi^*)} \\approx \\big( \\frac{W_{s_0,G}(\\pi, T) - W_{s_0,G}(\\pi_R, T)}{W_{s_0,G}(\\pi^*, T) - W_{s_0,G}(\\pi_R, T)} \\big)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Negative Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model of intelligence allows negative value. The intuition way of thinking about. It tells us how intelligent a system is towards the opposite goal function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization of Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand the relationship of intelligence between two agents with same configuration. We will try to generalize our model to compare systems with different settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T cliff in I-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Randomness and Enivornment dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negative intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Exchange layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections, we introduced a intelligence model assuming direct information exchange between the agent and its environment. \n",
    "\n",
    "\n",
    "Roughly speaking, we can outline the capability of a autonomous system as follow\n",
    "\n",
    "Capability = Sensitivity $\\times$ Intelligence $\\times$ Strength\n",
    "\n",
    "In order to address the above formula in an mathemetical way. We create a separate layer between the environment and the kernel of the agent, which named it exchange layer.\n",
    "\n",
    "1. $O$ - the space of raw observation\n",
    "2. $O^P$ - the space of perceviable obseravtion\n",
    "3. $A$ - the space of logic output\n",
    "4. $A^T$ - the space of actions which affect the environment\n",
    "\n",
    "We want to minimize the influence of **Sensitivity** and **Strength**, and only calculating the intelligence based on the logic part. \n",
    "\n",
    "\\begin{equation}\n",
    "o_i^p = P(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\prod,G\\}}(t) = E\\big(G(P_1(o_t))|a_{i1}^{},P_1(o_{i1})...a_{it}^{},P_1(o_{it})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\prod,G\\}}(t) = E\\big(G(P_2(o_t))|a_{j1}^{},P_2(o_{j1})...a_{it}^{},P_2(o_{jt})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The division between logic layer and exchange layer is basically determined by the viewpoint of the observer. Different divisions can be applied to the same (agent, environment) combination when the observation point changed. A human can be viewed as , or a single logic layer with certain assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Balance Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem . Two agent $\\pi_1, \\pi_2$ are considred to be balanced \n",
    "\n",
    "Textual defintion: Two autonomous system are considered as **Balanced** if their **Sensitivity** and **Strength** are the equivilent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Strict Sensation Equivalency\n",
    "\n",
    "Given ${O, P_1, P_2}$, there exists a $P'$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall o_i \\in O, P_1(o_i) = P' \\times P_2(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "Are we able to construct a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Strict Strength Equivalency\n",
    "\n",
    "Given ${O, A^T_1, A^T_2}$, there exists a $P'$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall o_i \\in O, P_1(o_i) = P' \\times P_2(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Are we able to construct a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Intelligent through partial credit and proof by Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of obtaining I-value through direct sampling. This section introduce some heuristics aim to solve the problem of identifying I-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Partial-Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a problem tuple ${\\{s_0, G, R, T\\}}$ and we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of optimals / total number of optimals\n",
    "\n",
    "calculate intelligence based on optimal action and optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Proof by Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important idea we can apply to the model is to look at the actions directly. Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Environement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $o_i$ - observation\n",
    "2. $G(o_{t})$ - Goal function\n",
    "3. $G_{t}^{*}$ - n-scope goal function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend our definition of I to include more fancy staff\n",
    "\n",
    "Why intelligent system needs to be optimal?\n",
    "\n",
    "No, it does not need to be optimal at all. That's why intelligence can be different.\n",
    "\n",
    "Upperbound and lowerbound of intelligence??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different systems with the same problem setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intent inference????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal policy?\n",
    "important sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
