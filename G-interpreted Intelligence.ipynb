{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to describe the design of G-interpreted intelligence.\n",
    "\n",
    "G-interpreted Intelligence is a black-box mathmetical model of intelligence. Intelligence through behavior interpretation and intention inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. G-interpreted Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many subfields of AI tries to solve the problem - how to make a intelligent agent. The objective of this paper is to bridge the gap between engineering approaches from RL and other field, and the search of a logic behind general intelligence. In this paper, we enbrace the natural idea described in reinforcement learning - the assumption that a intelligent system is an autonomous system pursuing optimization over its input space.\n",
    "\n",
    "In this case, we are trying to optimize according to the reward signal.\n",
    "\n",
    "On the other hand, raising concern about the interpretability of such complicated system. One question behind the screen is that, whether we are able to understand all different presentation of intelligence or not? What's the relationship between the observer and the intelligence which observed from another entity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 G function and G value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the objective function from Reinforcement Learning, where $\\theta^*$ is the optimal policy and $E_{\\tau \\sim p_\\theta(\\tau)}$ is the expected cumulative reward of trajectory $\\tau = \\{a_1, o_1,..., a_n, o_n\\}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} E_{\\tau \\sim p_\\theta(\\tau)}[ \\sum_{t}^{\\infty} r(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "In our case, instead of assuming there is always a single special signal which represents the incentive, we believe that an intelligent system can drive itself to the maximization of a goal value which transformed from a sequence of input. \n",
    "\n",
    "Thus, we can construct a function $G(o)$ to maximize. The goal function can be seen as the intention of a intelligent system. The identification of intelligence is equivalent to the search of the intention.\n",
    "\n",
    "**Assumption**\n",
    "\n",
    "**Textual Definition**: An intelligent system is a system which attempts to maximize its goal value\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} E_{\\tau \\sim p_\\theta(\\tau)}[G(o_t)]\n",
    "\\end{equation}\n",
    "\n",
    "Because we don't know about the underlying structure, any structure of agent can serve us welll if it meets the requirement we set up. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_{t} + \\alpha \\frac{d G}{d \\theta} G(o_{t})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 W value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the effort of the agent following policy $\\pi$ given time $t$, we can construct a $W$ value as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "W_{\\{s_0,G\\}}(\\pi，t) = E\\big(G(o_t) \\sim \\{ a_{1}^{\\pi},o_1...a_{t}^{\\pi},o_{t} \\} \\big). \n",
    "\\end{equation}\n",
    "\n",
    "$s_0$ - the inital state of the environment before the first observation $o_1$\n",
    "\n",
    "$\\pi$ - the observed agent defined by its policy $\\pi$\n",
    "\n",
    "Basically, the $W$ function tells us the expected $G$ value an agent $\\pi$ can achieve from state $s_0$ after time $t$. We use the policy $\\pi$ to identify different agent in order to keep our model independent from the underlying structure of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Identification of agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our definition of agent: \n",
    "\n",
    "Agent $\\prod$ and $\\prod '$ are identical if their policy $\\pi = \\pi '$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Zero-point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hardly discussed question in AI is - how we can tell something is not intelligent. In other word, the zero-point of intelligence is hardly discussed. How can we define its minimum if intelligence is a single-dimensioned metrics.\n",
    "\n",
    "The randomness of agent needs to be discussed when we know about the environment dynamics. But for now,let's assume we have no background knowledge of the environment and the agent at all and use a simple agent following random policy.\n",
    "\n",
    "Therefore, our baseline can be constructed as\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,G\\}}(\\pi_{R}，t) = E\\big(G(o_t) \\sim \\{ a_{1}^{\\pi_{R}},o_1...a_{t}^{\\pi_{R}},o_{t} \\} \\big). \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Definiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of intelligence can now be written in a mathemetical form. Since we only need to find an arbitary goal function. In order to tell if a system is intelligent or not, we just need to find one explanation of the behavior of the agent.\n",
    "\n",
    "**Theory**. For an observation starts at state $s_0$ and lasts for time $T$, a system following policy $\\pi$ is **Perceviably Intelligent** if there exists a Goal function $G$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,G\\}}(\\pi, T) - W_{\\{s_0,G\\}}(R, T) > \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $\\epsilon$ is the acceptance constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 I-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2, we describe the minimum requirement for a system to be intelligent. But the more interesting problem we need to answer is how we can compare two separate systems, or systems under different **metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if we follow the same logic from W-value, then it would be clear for us to define the comparable scalar of intelligence repect to the tuple $\\{s_0, T, G, R\\}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "I_{s_0, T, G, R}(\\pi) = (\\frac {\\Delta W_{s_0, G}}{\\Delta t})^+ \\approx \\frac{1}{T} \\big( W_{s_0,G}(\\pi, T) - W_{s_0,G}(R, T) \\big)^+\n",
    "\\end{equation}\n",
    "\n",
    "We call this scalar I-value and the ${\\{s_0, T, G, R\\}}$ as the **Metric**. We apply the $()^+$ function to ignore the negative part. Apparently, there are some problems with this over-simplified model. We will try to address some of the concerns in later section. The later sections of this paper attempts to generalize this model of intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.1 I-value with normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the optimal policy, we can normalize our I-value using the optimal policy\n",
    "\n",
    "\\begin{equation}\n",
    "I_{s_0, T, G, R}^{norm}(\\pi) = \\frac{I_{s_0, T, G, R}(\\pi)}{I_{s_0, T, G, R}(\\pi^*)} \\approx \\big( \\frac{W_{s_0,G}(\\pi, T) - W_{s_0,G}(\\pi_R, T)}{W_{s_0,G}(\\pi^*, T) - W_{s_0,G}(\\pi_R, T)} \\big)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Negative Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model of intelligence allows negative value. The intuition way of thinking about. It tells us how intelligent a system is towards the opposite goal function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. s_0 and T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first section we want to extend is the inital state and time T\n",
    "\n",
    "Strick evaluation of intelligence - only evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Balance Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Strict Sensation Equivalency\n",
    "\n",
    "Given ${O, P_1, P_2}$, there exists a $P'$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall o_i \\in O, P_1(o_i) = P' \\times P_2(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "Are we able to construct a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Strict Strength Equivalency\n",
    "\n",
    "Given ${O, A^T_1, A^T_2}$, there exists a $P'$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall o_i \\in O, P_1(o_i) = P' \\times P_2(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "Are we able to construct a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sampling heuristics - Task-oriented I-value, Partial credit and Prove by Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of obtaining I-value through direct sampling can be tedious. This section introduce some heuristics aim to solve the problem of identifying I-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Task-oriented I-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert graphh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The I-value introduced previously uses a strict time constrain $T$. In some cases, we may want to give the agent some more time until it can reach a certain milestone. Therefore, we can rewrite the I-value formula as:\n",
    "\n",
    "\\begin{equation}\n",
    "I_{s_0, s_1, G, R}(\\pi) = \\big( \\frac{G(o_1) - G(o_0) - W_{s_0,G}(R, T)}{E(T | s_0 \\overset{\\pi}{\\rightarrow} s_1)}  \\big)^+\n",
    "\\end{equation}\n",
    "\n",
    "$G(o_1) - G(o_0)$ - the **achievement** of agent between state $s_0$ and $s_1$, it is a constant determined before the observation\n",
    "\n",
    "$W_{s_0,G}(R, T)$ - the baseline using a random policy for time T, this a varaible we need sample.\n",
    "\n",
    "The good part of task-oriented Intelligence is that we only need to sample the baseline and expected time for finishing the task. These two sampling can be down saparately.\n",
    "\n",
    "${s_0 \\rightarrow s_1}$, the achievement is a constant once we decided the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Partial Crediting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial-credit is a method of evaluating intelligence by breaking the trajectory into several parts. The idea is that a trajectory may requires a fixed sequence of change in the environment. It doesn't matter how intelligent a system is, if the system wants to reach state B, it has to pass state A. Therefore, we can evaluate the I-value by evaluating known state intervals ${s_i, s'_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to set the weight is to set the weight as the timespan propotional to the optimal trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a **metrics** tuple ${\\{s_0, S, T, G, R\\}}$ and an agent following policy $\\pi$,\n",
    "\\begin{equation}\n",
    "I_{\\{s_0, S, T, G, R\\}} = \\sum_{i}{} \\beta_i \\big ( \\frac{  G(o_{i+1}) - G(o_{i}) - W_{s_0,G}(R, T_i) }{ E(T_i| s_0 \\overset{\\pi}{\\rightarrow} s_1)} \\big)^+\n",
    "\\end{equation}\n",
    "\n",
    "$s = \\{(s_0, s_1),(s_1, s_2),...,(s_i, s_{i+1})\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Prove-through-Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove-through-Reasoning(**PtR**) is a method aims to reduce the variance of I-value evaluation assuming some of the outcomes of actions is known to the observer.\n",
    "\n",
    "In PtR, the observe is able to reason the quality of **action** instead of evaluating the outcome $s_t$\n",
    "\n",
    "Given the a set of tuples ${S, A, O}$, if we know the state and and action, we can tell that the observation at next iteration.\n",
    "\n",
    "Suppose we have a transition tuple $\\delta = \\{(s_1, a_1, s_2),...,(s_i, a_i, s_{i+1})\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Partial observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between $o_i$ and $s_i$, we will discuss thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization of Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand the relationship of intelligence between two agents with same configuration.\n",
    "\n",
    "The first goal here is to find out the relationships of intelligence under different metrics, same agent - different environment\n",
    "\n",
    "The second goal of generalization. One criticial aspect of intelligence is that it allows a system to perform well in all different environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the problem we are try to figure out is given $I$, $s_0 \\sim E$ and $s_0^\\prime \\sim E^\\prime$, what can we infer $I^\\prime$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Intelligence inference using Prove-through-Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Balancing Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections, we introduced a intelligence model assuming direct information exchange between the agent and its environment. \n",
    "\n",
    "\n",
    "Roughly speaking, we can outline the capability of a autonomous system as follow\n",
    "\n",
    "Capability = Sensitivity $\\times$ Intelligence $\\times$ Strength\n",
    "\n",
    "In order to address the above formula in an mathemetical way. We create a separate layer between the environment and the kernel of the agent, which named it exchange layer.\n",
    "\n",
    "1. $O$ - the space of raw observation\n",
    "2. $O^P$ - the space of perceviable obseravtion\n",
    "3. $A$ - the space of logic output\n",
    "4. $A^T$ - the space of actions which affect the environment\n",
    "\n",
    "We want to minimize the influence of **Sensitivity** and **Strength**, and only calculating the intelligence based on the logic part. \n",
    "\n",
    "\\begin{equation}\n",
    "o_i^p = P(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\prod,G\\}}(t) = E\\big(G(P_1(o_t))|a_{i1}^{},P_1(o_{i1})...a_{it}^{},P_1(o_{it})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\prod,G\\}}(t) = E\\big(G(P_2(o_t))|a_{j1}^{},P_2(o_{j1})...a_{it}^{},P_2(o_{jt})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The division between logic layer and exchange layer is basically determined by the viewpoint of the observer. Different divisions can be applied to the same (agent, environment) combination when the observation point changed. A human can be viewed as , or a single logic layer with certain assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.  Exchange layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write down the balancing condition as:\n",
    "\n",
    "Textual defintion: In environment $E$, Two agent starts at state $s_0$ are considered as **Balanced** if their **Sensitivity** and **Strength** are the equivilent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Environement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $o_i$ - observation\n",
    "2. $G(o_{t})$ - Goal function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend our definition of I to include more fancy staff\n",
    "\n",
    "Why intelligent system needs to be optimal?\n",
    "\n",
    "No, it does not need to be optimal at all. That's why intelligence can be different.\n",
    "\n",
    "Upperbound and lowerbound of intelligence??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different systems with the same problem setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intent inference????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal policy?\n",
    "important sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long term planning???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
