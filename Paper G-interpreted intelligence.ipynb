{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstraction - breifly say what you've done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper introduces a mathematical definition of general intelligence in Markov Decision Processes. We consider intelligence as a scalar quantity that measures the effort accomplished by an agent per time cycle; while the effort is a linear transformation (G-function) of the agent's last observation. Instead of deriving the generalized model at once, we first construct a model of intelligence in terms of a specific metrics tuple. Then we approach the problem of generality by discussing the equivalence conditions in each corresponding components of the metrics. The evaluation of G-Interpreted Intelligence (G-II) requires neither knowledge about the agents' underlying structure nor their current state. In addition, we propose several heuristics for optimizing the process of sampling and an algorithm for constructing and comparing G-IIs from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People have found a link \n",
    "\n",
    "The debatable definition of intelligence\n",
    "\n",
    "We introduce a noval definition.\n",
    "\n",
    "These definitions are hardly computable or comparable.\n",
    "\n",
    "So comes the criticism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannot be applied to the engineered narrow-task agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to ignore the effort people have spent on attempting to reach a concensus on the definition of intelligence . and many other desiciplines . The major problem of these definition is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G-function, such function has been descirbed in inverse reinforcement learning as well.\n",
    "\n",
    "We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional RL, policy denotes to a probability distribution over action space given current state. In our model, we extend the definition of policy to include the agent's future potentials as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
