{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild HW3 - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dqn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-83538bbaacc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#from dqn_utils import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0matari_wrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dqn'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import dqn\n",
    "#from dqn_utils import *\n",
    "from atari_wrappers import *\n",
    "\n",
    "\n",
    "# def atari_model(img_in, num_actions, scope, reuse=False):\n",
    "#     # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "#     with tf.variable_scope(scope, reuse=reuse):\n",
    "#         out = img_in\n",
    "#         with tf.variable_scope(\"convnet\"):\n",
    "#             # original architecture\n",
    "#             out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "#             out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "#             out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "#         out = layers.flatten(out)\n",
    "#         with tf.variable_scope(\"action_value\"):\n",
    "#             out = layers.fully_connected(out, num_outputs=512,         activation_fn=tf.nn.relu)\n",
    "#             out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# def atari_learn(env,\n",
    "#                 session,\n",
    "#                 num_timesteps):\n",
    "#     # This is just a rough estimate\n",
    "#     num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "#     lr_multiplier = 1.0\n",
    "#     lr_schedule = PiecewiseSchedule([\n",
    "#                                          (0,                   1e-4 * lr_multiplier),\n",
    "#                                          (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "#                                          (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "#                                     ],\n",
    "#                                     outside_value=5e-5 * lr_multiplier)\n",
    "#     optimizer = dqn.OptimizerSpec(\n",
    "#         constructor=tf.train.AdamOptimizer,\n",
    "#         kwargs=dict(epsilon=1e-4),\n",
    "#         lr_schedule=lr_schedule\n",
    "#     )\n",
    "\n",
    "#     def stopping_criterion(env, t):\n",
    "#         # notice that here t is the number of steps of the wrapped env,\n",
    "#         # which is different from the number of steps in the underlying env\n",
    "#         return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "\n",
    "#     exploration_schedule = PiecewiseSchedule(\n",
    "#         [\n",
    "#             (0, 1.0),\n",
    "#             (1e6, 0.1),\n",
    "#             (num_iterations / 2, 0.01),\n",
    "#         ], outside_value=0.01\n",
    "#     )\n",
    "\n",
    "#     dqn.learn(\n",
    "#         env=env,\n",
    "#         q_func=atari_model,\n",
    "#         optimizer_spec=optimizer,\n",
    "#         session=session,\n",
    "#         exploration=exploration_schedule,\n",
    "#         stopping_criterion=stopping_criterion,\n",
    "#         replay_buffer_size=1000000,\n",
    "#         batch_size=32,\n",
    "#         gamma=0.99,\n",
    "#         learning_starts=50000,\n",
    "#         learning_freq=4,\n",
    "#         frame_history_len=4,\n",
    "#         target_update_freq=10000,\n",
    "#         grad_norm_clipping=10,\n",
    "#         double_q=True\n",
    "#     )\n",
    "#     env.close()\n",
    "\n",
    "# def get_available_gpus():\n",
    "#     from tensorflow.python.client import device_lib\n",
    "#     local_device_protos = device_lib.list_local_devices()\n",
    "#     return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "# def set_global_seeds(i):\n",
    "#     try:\n",
    "#         import tensorflow as tf\n",
    "#     except ImportError:\n",
    "#         pass\n",
    "#     else:\n",
    "#         tf.set_random_seed(i)\n",
    "#     np.random.seed(i)\n",
    "#     random.seed(i)\n",
    "\n",
    "# def get_session():\n",
    "#     tf.reset_default_graph()\n",
    "#     tf_config = tf.ConfigProto(\n",
    "#         inter_op_parallelism_threads=1,\n",
    "#         intra_op_parallelism_threads=1)\n",
    "#     session = tf.Session(config=tf_config)\n",
    "#     print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "#     return session\n",
    "\n",
    "# def get_env(task, seed):\n",
    "#     env = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "#     set_global_seeds(seed)\n",
    "#     env.seed(seed)\n",
    "\n",
    "#     expt_dir = '/tmp/hw3_vid_dir2/'\n",
    "#     env = wrappers.Monitor(env, osp.join(expt_dir, \"gym\"), force=True)\n",
    "#     env = wrap_deepmind(env)\n",
    "\n",
    "#     return env\n",
    "\n",
    "# def main():\n",
    "#     # Get Atari games.\n",
    "#     task = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "#     # Run training\n",
    "#     seed = random.randint(0, 9999)\n",
    "#     print('random seed = %d' % seed)\n",
    "#     env = get_env(task, seed)\n",
    "#     session = get_session()\n",
    "#     atari_learn(env, session, num_timesteps=2e8)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Atatri Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        super(NoopResetEnv, self).__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset()\n",
    "        noops = np.random.randint(1, self.noop_max + 1)\n",
    "        for _ in range(noops):\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        return obs\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def _reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, _, _ = self.env.step(1)\n",
    "        obs, _, _, _ = self.env.step(2)\n",
    "        return obs\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        super(EpisodicLifeEnv, self).__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "        self.was_real_reset = False\n",
    "\n",
    "    def _step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset()\n",
    "            self.was_real_reset = True\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "            self.was_real_reset = False\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def _step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "def _process_frame84(frame):\n",
    "    img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "    resized_screen = cv2.resize(img, (84, 110),  interpolation=cv2.INTER_LINEAR)\n",
    "    x_t = resized_screen[18:102, :]\n",
    "    x_t = np.reshape(x_t, [84, 84, 1])\n",
    "    return x_t.astype(np.uint8)\n",
    "\n",
    "class ProcessFrame84(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
    "\n",
    "    def _step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return _process_frame84(obs), reward, done, info\n",
    "\n",
    "    def _reset(self):\n",
    "        return _process_frame84(self.env.reset())\n",
    "\n",
    "class ClippedRewardsWrapper(gym.Wrapper):\n",
    "    def _step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, np.sign(reward), done, info\n",
    "\n",
    "def wrap_deepmind_ram(env):\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ClippedRewardsWrapper(env)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind(env):\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ClippedRewardsWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DQN Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file includes a collection of utility functions that are useful for\n",
    "implementing DQN.\"\"\"\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def huber_loss(x, delta=1.0):\n",
    "    # https://en.wikipedia.org/wiki/Huber_loss\n",
    "    return tf.where(\n",
    "        tf.abs(x) < delta,\n",
    "        tf.square(x) * 0.5,\n",
    "        delta * (tf.abs(x) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "def sample_n_unique(sampling_f, n):\n",
    "    \"\"\"Helper function. Given a function `sampling_f` that returns\n",
    "    comparable objects, sample n such unique objects.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    while len(res) < n:\n",
    "        candidate = sampling_f()\n",
    "        if candidate not in res:\n",
    "            res.append(candidate)\n",
    "    return res\n",
    "\n",
    "class Schedule(object):\n",
    "    def value(self, t):\n",
    "        \"\"\"Value of the schedule at time t\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class ConstantSchedule(object):\n",
    "    def __init__(self, value):\n",
    "        \"\"\"Value remains constant over time.\n",
    "        Parameters\n",
    "        ----------\n",
    "        value: float\n",
    "            Constant value of the schedule\n",
    "        \"\"\"\n",
    "        self._v = value\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        return self._v\n",
    "\n",
    "def linear_interpolation(l, r, alpha):\n",
    "    return l + alpha * (r - l)\n",
    "\n",
    "class PiecewiseSchedule(object):\n",
    "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
    "        \"\"\"Piecewise schedule.\n",
    "        endpoints: [(int, int)]\n",
    "            list of pairs `(time, value)` meanining that schedule should output\n",
    "            `value` when `t==time`. All the values for time must be sorted in\n",
    "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
    "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
    "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
    "            time passed between `time_a` and `time_b` for time `t`.\n",
    "        interpolation: lambda float, float, float: float\n",
    "            a function that takes value to the left and to the right of t according\n",
    "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
    "            right endpoint that t has covered. See linear_interpolation for example.\n",
    "        outside_value: float\n",
    "            if the value is requested outside of all the intervals sepecified in\n",
    "            `endpoints` this value is returned. If None then AssertionError is\n",
    "            raised when outside value is requested.\n",
    "        \"\"\"\n",
    "        idxes = [e[0] for e in endpoints]\n",
    "        assert idxes == sorted(idxes)\n",
    "        self._interpolation = interpolation\n",
    "        self._outside_value = outside_value\n",
    "        self._endpoints      = endpoints\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
    "            if l_t <= t and t < r_t:\n",
    "                alpha = float(t - l_t) / (r_t - l_t)\n",
    "                return self._interpolation(l, r, alpha)\n",
    "\n",
    "        # t does not belong to any of the pieces, so doom.\n",
    "        assert self._outside_value is not None\n",
    "        return self._outside_value\n",
    "\n",
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        \"\"\"Linear interpolation between initial_p and final_p over\n",
    "        schedule_timesteps. After this many timesteps pass final_p is\n",
    "        returned.\n",
    "        Parameters\n",
    "        ----------\n",
    "        schedule_timesteps: int\n",
    "            Number of timesteps for which to linearly anneal initial_p\n",
    "            to final_p\n",
    "        initial_p: float\n",
    "            initial output value\n",
    "        final_p: float\n",
    "            final output value\n",
    "        \"\"\"\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p            = final_p\n",
    "        self.initial_p          = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        fraction  = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
    "\n",
    "def compute_exponential_averages(variables, decay):\n",
    "    \"\"\"Given a list of tensorflow scalar variables\n",
    "    create ops corresponding to their exponential\n",
    "    averages\n",
    "    Parameters\n",
    "    ----------\n",
    "    variables: [tf.Tensor]\n",
    "        List of scalar tensors.\n",
    "    Returns\n",
    "    -------\n",
    "    averages: [tf.Tensor]\n",
    "        List of scalar tensors corresponding to averages\n",
    "        of al the `variables` (in order)\n",
    "    apply_op: tf.runnable\n",
    "        Op to be run to update the averages with current value\n",
    "        of variables.\n",
    "    \"\"\"\n",
    "    averager = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "    apply_op = averager.apply(variables)\n",
    "    return [averager.average(v) for v in variables], apply_op\n",
    "\n",
    "def minimize_and_clip(optimizer, objective, var_list, clip_val=10):\n",
    "    \"\"\"Minimized `objective` using `optimizer` w.r.t. variables in\n",
    "    `var_list` while ensure the norm of the gradients for each\n",
    "    variable is clipped to `clip_val`\n",
    "    \"\"\"\n",
    "    gradients = optimizer.compute_gradients(objective, var_list=var_list)\n",
    "    for i, (grad, var) in enumerate(gradients):\n",
    "        if grad is not None:\n",
    "            gradients[i] = (tf.clip_by_norm(grad, clip_val), var)\n",
    "    return optimizer.apply_gradients(gradients)\n",
    "\n",
    "def initialize_interdependent_variables(session, vars_list, feed_dict):\n",
    "    \"\"\"Initialize a list of variables one at a time, which is useful if\n",
    "    initialization of some variables depends on initialization of the others.\n",
    "    \"\"\"\n",
    "    vars_left = vars_list\n",
    "    while len(vars_left) > 0:\n",
    "        new_vars_left = []\n",
    "        for v in vars_left:\n",
    "            try:\n",
    "                # If using an older version of TensorFlow, uncomment the line\n",
    "                # below and comment out the line after it.\n",
    "\t\t#session.run(tf.initialize_variables([v]), feed_dict)\n",
    "                session.run(tf.variables_initializer([v]), feed_dict)\n",
    "            except tf.errors.FailedPreconditionError:\n",
    "                new_vars_left.append(v)\n",
    "        if len(new_vars_left) >= len(vars_left):\n",
    "            # This can happend if the variables all depend on each other, or more likely if there's\n",
    "            # another variable outside of the list, that still needs to be initialized. This could be\n",
    "            # detected here, but life's finite.\n",
    "            raise Exception(\"Cycle in variable dependencies, or extenrnal precondition unsatisfied.\")\n",
    "        else:\n",
    "            vars_left = new_vars_left\n",
    "\n",
    "def get_wrapper_by_name(env, classname):\n",
    "    currentenv = env\n",
    "    while True:\n",
    "        if classname in currentenv.__class__.__name__:\n",
    "            return currentenv\n",
    "        elif isinstance(env, gym.Wrapper):\n",
    "            currentenv = currentenv.env\n",
    "        else:\n",
    "            raise ValueError(\"Couldn't find wrapper named %s\"%classname)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, frame_history_len, lander=False):\n",
    "        \"\"\"This is a memory efficient implementation of the replay buffer.\n",
    "\n",
    "        The sepecific memory optimizations use here are:\n",
    "            - only store each frame once rather than k times\n",
    "              even if every observation normally consists of k last frames\n",
    "            - store frames as np.uint8 (actually it is most time-performance\n",
    "              to cast them back to float32 on GPU to minimize memory transfer\n",
    "              time)\n",
    "            - store frame_t and frame_(t+1) in the same buffer.\n",
    "\n",
    "        For the tipical use case in Atari Deep RL buffer with 1M frames the total\n",
    "        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n",
    "\n",
    "        Warning! Assumes that returning frame of zeros at the beginning\n",
    "        of the episode, when there is less frames than `frame_history_len`,\n",
    "        is acceptable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        frame_history_len: int\n",
    "            Number of memories to be retried for each observation.\n",
    "        \"\"\"\n",
    "        self.lander = lander\n",
    "\n",
    "        self.size = size\n",
    "        self.frame_history_len = frame_history_len\n",
    "\n",
    "        self.next_idx      = 0\n",
    "        self.num_in_buffer = 0\n",
    "\n",
    "        self.obs      = None\n",
    "        self.action   = None\n",
    "        self.reward   = None\n",
    "        self.done     = None\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
    "        return batch_size + 1 <= self.num_in_buffer\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obs_batch      = np.concatenate([self._encode_observation(idx)[None] for idx in idxes], 0)\n",
    "        act_batch      = self.action[idxes]\n",
    "        rew_batch      = self.reward[idxes]\n",
    "        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[None] for idx in idxes], 0)\n",
    "        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
    "\n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample `batch_size` different transitions.\n",
    "\n",
    "        i-th sample transition is the following:\n",
    "\n",
    "        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n",
    "        after which reward `rew_batch[i]` was received and subsequent\n",
    "        observation  next_obs_batch[i] was observed, unless the epsiode\n",
    "        was done which is represented by `done_mask[i]` which is equal\n",
    "        to 1 if episode has ended as a result of that action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8\n",
    "        act_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "        rew_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        next_obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8\n",
    "        done_mask: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        \"\"\"\n",
    "        assert self.can_sample(batch_size)\n",
    "        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "    def encode_recent_observation(self):\n",
    "        \"\"\"Return the most recent `frame_history_len` frames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observation: np.array\n",
    "            Array of shape (img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n",
    "            encodes frame at time `t - frame_history_len + i`\n",
    "        \"\"\"\n",
    "        assert self.num_in_buffer > 0\n",
    "        return self._encode_observation((self.next_idx - 1) % self.size)\n",
    "\n",
    "    def _encode_observation(self, idx):\n",
    "        end_idx   = idx + 1 # make noninclusive\n",
    "        start_idx = end_idx - self.frame_history_len\n",
    "        # this checks if we are using low-dimensional observations, such as RAM\n",
    "        # state, in which case we just directly return the latest RAM.\n",
    "        if len(self.obs.shape) == 2:\n",
    "            return self.obs[end_idx-1]\n",
    "        # if there weren't enough frames ever in the buffer for context\n",
    "        if start_idx < 0 and self.num_in_buffer != self.size:\n",
    "            start_idx = 0\n",
    "        for idx in range(start_idx, end_idx - 1):\n",
    "            if self.done[idx % self.size]:\n",
    "                start_idx = idx + 1\n",
    "        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "        # if zero padding is needed for missing context\n",
    "        # or we are on the boundry of the buffer\n",
    "        if start_idx < 0 or missing_context > 0:\n",
    "            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                frames.append(self.obs[idx % self.size])\n",
    "            return np.concatenate(frames, 2)\n",
    "        else:\n",
    "            # this optimization has potential to saves about 30% compute time \\o/\n",
    "            img_h, img_w = self.obs.shape[1], self.obs.shape[2]\n",
    "            return self.obs[start_idx:end_idx].transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "    def store_frame(self, frame):\n",
    "        \"\"\"Store a single frame in the buffer at the next available index, overwriting\n",
    "        old frames if necessary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: np.array\n",
    "            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
    "            the frame to be stored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            Index at which the frame is stored. To be used for `store_effect` later.\n",
    "        \"\"\"\n",
    "        if self.obs is None:\n",
    "            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.float32 if self.lander else np.uint8)\n",
    "            self.action   = np.empty([self.size],                     dtype=np.int32)\n",
    "            self.reward   = np.empty([self.size],                     dtype=np.float32)\n",
    "            self.done     = np.empty([self.size],                     dtype=np.bool)\n",
    "        self.obs[self.next_idx] = frame\n",
    "\n",
    "        ret = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1) % self.size\n",
    "        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def store_effect(self, idx, action, reward, done):\n",
    "        \"\"\"Store effects of action taken after obeserving frame stored\n",
    "        at index idx. The reason `store_frame` and `store_effect` is broken\n",
    "        up into two functions is so that once can call `encode_recent_observation`\n",
    "        in between.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        idx: int\n",
    "            Index in buffer of recently observed frame (returned by `store_frame`).\n",
    "        action: int\n",
    "            Action that was performed upon observing this frame.\n",
    "        reward: float\n",
    "            Reward that was received when the actions was performed.\n",
    "        done: bool\n",
    "            True if episode was finished after performing that action.\n",
    "        \"\"\"\n",
    "        self.action[idx] = action\n",
    "        self.reward[idx] = reward\n",
    "        self.done[idx]   = done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DQN Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dqn_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d4d8dbacbc16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdqn_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mOptimizerSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OptimizerSpec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"constructor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr_schedule\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dqn_utils'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from dqn_utils import *\n",
    "\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])\n",
    "\n",
    "class QLearner(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        q_func,\n",
    "        optimizer_spec,\n",
    "        session,\n",
    "        exploration=LinearSchedule(1000000, 0.1),\n",
    "        stopping_criterion=None,\n",
    "        replay_buffer_size=1000000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        learning_starts=50000,\n",
    "        learning_freq=4,\n",
    "        frame_history_len=4,\n",
    "        target_update_freq=10000,\n",
    "        grad_norm_clipping=10,\n",
    "        rew_file=None,\n",
    "        double_q=True,\n",
    "        lander=False):\n",
    "        \"\"\"Run Deep Q-learning algorithm.\n",
    "        \n",
    "        You can specify your own convnet using q_func.\n",
    "        \n",
    "        All schedules are w.r.t. total number of steps taken in the environment.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env: gym.Env\n",
    "        gym environment to train on.\n",
    "        q_func: function\n",
    "            Model to use for computing the q function. It should accept the\n",
    "                following named arguments:\n",
    "                img_in: tf.Tensor\n",
    "                    tensorflow tensor representing the input image\n",
    "                num_actions: int\n",
    "                    number of actions\n",
    "                scope: str\n",
    "                    scope in which all the model related variables\n",
    "                    should be created\n",
    "                reuse: bool\n",
    "                    whether previously created variables should be reused.\n",
    "        optimizer_spec: OptimizerSpec\n",
    "            Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "            for the optimizer\n",
    "        session: tf.Session\n",
    "            tensorflow session to use.\n",
    "        exploration: rl_algs.deepq.utils.schedules.Schedule\n",
    "            schedule for probability of chosing random action.\n",
    "        stopping_criterion: (env, t) -> bool\n",
    "            should return true when it's ok for the RL algorithm to stop.\n",
    "            takes in env and the number of steps executed so far.\n",
    "        replay_buffer_size: int\n",
    "            How many memories to store in the replay buffer.\n",
    "        batch_size: int\n",
    "            How many transitions to sample each time experience is replayed.\n",
    "        gamma: float\n",
    "            Discount Factor\n",
    "        learning_starts: int\n",
    "            After how many environment steps to start replaying experiences\n",
    "        learning_freq: int\n",
    "            How many steps of environment to take between every experience replay\n",
    "        frame_history_len: int\n",
    "            How many past frames to include as input to the model.\n",
    "        target_update_freq: int\n",
    "            How many experience replay rounds (not steps!) to perform between\n",
    "            each update to the target Q network\n",
    "        grad_norm_clipping: float or None\n",
    "            If not None gradients' norms are clipped to this value.\n",
    "        double_q: bool\n",
    "            If True, then use double Q-learning to compute target values. Otherwise, use vanilla DQN.\n",
    "            https://papers.nips.cc/paper/3964-double-q-learning.pdf\n",
    "    \"\"\"\n",
    "        assert type(env.observation_space) == gym.spaces.Box\n",
    "        assert type(env.action_space)      == gym.spaces.Discrete\n",
    "        \n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.optimizer_spec = optimizer_spec\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_freq = learning_freq\n",
    "        self.learning_starts = learning_starts\n",
    "        self.stopping_criterion = stopping_criterion\n",
    "        self.env = env\n",
    "        self.session = session\n",
    "        self.exploration = exploration\n",
    "        self.rew_file = str(uuid.uuid4()) + '.pkl' if rew_file is None else rew_file\n",
    "\n",
    "        ###############\n",
    "        # BUILD MODEL #\n",
    "        ###############\n",
    "\n",
    "        if len(self.env.observation_space.shape) == 1:\n",
    "            # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "            input_shape = self.env.observation_space.shape\n",
    "        else:\n",
    "            img_h, img_w, img_c = self.env.observation_space.shape\n",
    "            input_shape = (img_h, img_w, frame_history_len * img_c)\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set up placeholders\n",
    "        # placeholder for current observation (or state)\n",
    "        self.obs_t_ph              = tf.placeholder(\n",
    "            tf.float32 if lander else tf.uint8, [None] + list(input_shape))\n",
    "        # placeholder for current action\n",
    "        self.act_t_ph              = tf.placeholder(tf.int32,   [None])\n",
    "        # placeholder for current reward\n",
    "        self.rew_t_ph              = tf.placeholder(tf.float32, [None])\n",
    "        # placeholder for next observation (or state)\n",
    "        self.obs_tp1_ph            = tf.placeholder(\n",
    "            tf.float32 if lander else tf.uint8, [None] + list(input_shape))\n",
    "        # placeholder for end of episode mask\n",
    "        # this value is 1 if the next state corresponds to the end of an episode,\n",
    "        # in which case there is no Q-value at the next state; at the end of an\n",
    "        # episode, only the current state reward contributes to the target, not the\n",
    "        # next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "        self.done_mask_ph          = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # casting to float on GPU ensures lower data transfer times.\n",
    "        if lander:\n",
    "            obs_t_float = self.obs_t_ph\n",
    "            obs_tp1_float = self.obs_tp1_ph\n",
    "        else:\n",
    "            obs_t_float   = tf.cast(self.obs_t_ph,   tf.float32) / 255.0\n",
    "            obs_tp1_float = tf.cast(self.obs_tp1_ph, tf.float32) / 255.0\n",
    "\n",
    "        # Here, you should fill in your own code to compute the Bellman error. This requires\n",
    "        # evaluating the current and next Q-values and constructing the corresponding error.\n",
    "        # TensorFlow will differentiate this error for you, you just need to pass it to the\n",
    "        # optimizer. See assignment text for details.\n",
    "        # Your code should produce one scalar-valued tensor: total_error\n",
    "        # This will be passed to the optimizer in the provided code below.\n",
    "        # Your code should also produce two collections of variables:\n",
    "        # q_func_vars\n",
    "        # target_q_func_vars\n",
    "        # These should hold all of the variables of the Q-function network and target network,\n",
    "        # respectively. A convenient way to get these is to make use of TF's \"scope\" feature.\n",
    "        # For example, you can create your Q-function network with the scope \"q_func\" like this:\n",
    "        # <something> = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "        # And then you can obtain the variables like this:\n",
    "        # q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "        # Older versions of TensorFlow may require using \"VARIABLES\" instead of \"GLOBAL_VARIABLES\"\n",
    "        # Tip: use huber_loss (from dqn_utils) instead of squared error when defining self.total_error\n",
    "        ######\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        ######\n",
    "\n",
    "        # construct optimization op (with gradient clipping)\n",
    "        self.learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "        optimizer = self.optimizer_spec.constructor(learning_rate=self.learning_rate, **self.optimizer_spec.kwargs)\n",
    "        self.train_fn = minimize_and_clip(optimizer, self.total_error,\n",
    "                                          var_list=q_func_vars, clip_val=grad_norm_clipping)\n",
    "\n",
    "        # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "        update_target_fn = []\n",
    "        for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "            update_target_fn.append(var_target.assign(var))\n",
    "        self.update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "        # construct the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len, lander=lander)\n",
    "        self.replay_buffer_idx = None\n",
    "\n",
    "        ###############\n",
    "        # RUN ENV     #\n",
    "        ###############\n",
    "        self.model_initialized = False\n",
    "        self.num_param_updates = 0\n",
    "        self.mean_episode_reward      = -float('nan')\n",
    "        self.best_mean_episode_reward = -float('inf')\n",
    "        self.last_obs = self.env.reset()\n",
    "        self.log_every_n_steps = 10000\n",
    "\n",
    "        self.start_time = None\n",
    "        self.t = 0\n",
    "        \n",
    "        \n",
    "    def stopping_criterion_met(self):\n",
    "        return self.stopping_criterion is not None and self.stopping_criterion(self.env, self.t)\n",
    "\n",
    "    def step_env(self):\n",
    "        ### 2. Step the env and store the transition\n",
    "        # At this point, \"self.last_obs\" contains the latest observation that was\n",
    "        # recorded from the simulator. Here, your code needs to store this\n",
    "        # observation and its outcome (reward, next observation, etc.) into\n",
    "        # the replay buffer while stepping the simulator forward one step.\n",
    "        # At the end of this block of code, the simulator should have been\n",
    "        # advanced one step, and the replay buffer should contain one more\n",
    "        # transition.\n",
    "        # Specifically, self.last_obs must point to the new latest observation.\n",
    "        # Useful functions you'll need to call:\n",
    "        # obs, reward, done, info = env.step(action)\n",
    "        # this steps the environment forward one step\n",
    "        # obs = env.reset()\n",
    "        # this resets the environment if you reached an episode boundary.\n",
    "        # Don't forget to call env.reset() to get a new observation if done\n",
    "        # is true!!\n",
    "        # Note that you cannot use \"self.last_obs\" directly as input\n",
    "        # into your network, since it needs to be processed to include context\n",
    "        # from previous frames. You should check out the replay buffer\n",
    "        # implementation in dqn_utils.py to see what functionality the replay\n",
    "        # buffer exposes. The replay buffer has a function called\n",
    "        # encode_recent_observation that will take the latest observation\n",
    "        # that you pushed into the buffer and compute the corresponding\n",
    "        # input that should be given to a Q network by appending some\n",
    "        # previous frames.\n",
    "        # Don't forget to include epsilon greedy exploration!\n",
    "        # And remember that the first time you enter this loop, the model\n",
    "        # may not yet have been initialized (but of course, the first step\n",
    "        # might as well be random, since you haven't trained your net...)\n",
    "\n",
    "        #####\n",
    "        a = a + 1\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        ### 3. Perform experience replay and train the network.\n",
    "        # note that this is only done if the replay buffer contains enough samples\n",
    "        # for us to learn something useful -- until then, the model will not be\n",
    "        # initialized and random actions should be taken\n",
    "        if (self.t > self.learning_starts and \\\n",
    "            self.t % self.learning_freq == 0 and \\\n",
    "            self.replay_buffer.can_sample(self.batch_size)):\n",
    "        # Here, you should perform training. Training consists of four steps:\n",
    "        # 3.a: use the replay buffer to sample a batch of transitions (see the\n",
    "        # replay buffer code for function definition, each batch that you sample\n",
    "        # should consist of current observations, current actions, rewards,\n",
    "        # next observations, and done indicator).\n",
    "        # 3.b: initialize the model if it has not been initialized yet; to do\n",
    "        # that, call\n",
    "        #    initialize_interdependent_variables(self.session, tf.global_variables(), {\n",
    "        #        self.obs_t_ph: obs_t_batch,\n",
    "        #        self.obs_tp1_ph: obs_tp1_batch,\n",
    "        #    })\n",
    "        # where obs_t_batch and obs_tp1_batch are the batches of observations at\n",
    "        # the current and next time step. The boolean variable model_initialized\n",
    "        # indicates whether or not the model has been initialized.\n",
    "        # Remember that you have to update the target network too (see 3.d)!\n",
    "        # 3.c: train the model. To do this, you'll need to use the self.train_fn and\n",
    "        # self.total_error ops that were created earlier: self.total_error is what you\n",
    "        # created to compute the total Bellman error in a batch, and self.train_fn\n",
    "        # will actually perform a gradient step and update the network parameters\n",
    "        # to reduce total_error. When calling self.session.run on these you'll need to\n",
    "        # populate the following placeholders:\n",
    "        # self.obs_t_ph\n",
    "        # self.act_t_ph\n",
    "        # self.rew_t_ph\n",
    "        # self.obs_tp1_ph\n",
    "        # self.done_mask_ph\n",
    "        # (this is needed for computing self.total_error)\n",
    "        # self.learning_rate -- you can get this from self.optimizer_spec.lr_schedule.value(t)\n",
    "        # (this is needed by the optimizer to choose the learning rate)\n",
    "        # 3.d: periodically update the target network by calling\n",
    "        # self.session.run(self.update_target_fn)\n",
    "        # you should update every target_update_freq steps, and you may find the\n",
    "        # variable self.num_param_updates useful for this (it was initialized to 0)\n",
    "        #####\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "            self.num_param_updates += 1\n",
    "        self.t += 1\n",
    "    \n",
    "    def log_progress(self):\n",
    "        episode_rewards = get_wrapper_by_name(self.env, \"Monitor\").get_episode_rewards()\n",
    "        \n",
    "        if len(episode_rewards) > 0:\n",
    "            self.mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "\n",
    "        if len(episode_rewards) > 100:\n",
    "            self.best_mean_episode_reward = max(self.best_mean_episode_reward, self.mean_episode_reward)\n",
    "\n",
    "        if self.t % self.log_every_n_steps == 0 and self.model_initialized:\n",
    "            print(\"Timestep %d\" % (self.t,))\n",
    "            print(\"mean reward (100 episodes) %f\" % self.mean_episode_reward)\n",
    "            print(\"best mean reward %f\" % self.best_mean_episode_reward)\n",
    "            print(\"episodes %d\" % len(episode_rewards))\n",
    "            print(\"exploration %f\" % self.exploration.value(self.t))\n",
    "            print(\"learning_rate %f\" % self.optimizer_spec.lr_schedule.value(self.t))\n",
    "        if self.start_time is not None:\n",
    "            print(\"running time %f\" % ((time.time() - self.start_time) / 60.))\n",
    "            \n",
    "        self.start_time = time.time()\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        with open(self.rew_file, 'wb') as f:\n",
    "            pickle.dump(episode_rewards, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def learn(*args, **kwargs):\n",
    "        alg = QLearner(*args, **kwargs)\n",
    "        while not alg.stopping_criterion_met():\n",
    "            alg.step_env()\n",
    "            # at this point, the environment should have been advanced one step (and\n",
    "            # reset if done was true), and self.last_obs should point to the new latest\n",
    "            # observation\n",
    "            alg.update_model()\n",
    "            alg.log_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
