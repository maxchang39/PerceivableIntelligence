{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to describe the design of Goal-interpreted intelligence.\n",
    "\n",
    "Perceivable Intelligence is a mathmetical model of black-box intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perceivable Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many subfields of AI tries to solve the problem - how to make a intelligent agent. The objective of this paper is to bridge the gap between engineering approaches from RL and other field, and the search of a logic behind general intelligence. In this paper, we enbrace the natural idea described in reinforcement learning - the assumption that a intelligent system is an autonomous system pursuing optimization over its input space.\n",
    "\n",
    "In this case, we are trying to optimize according to the reward signal.\n",
    "\n",
    "On the other hand, raising concern about the interpretability of such complicated system. One question behind the screen is that, whether we are able to understand all different presentation of intelligence or not? What's the relationship between the observer and the intelligence which observed from another entity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 G function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the objective function from Reinforcement Learning, where $\\theta^*$ is the optimal policy and $E_{\\tau \\sim p_\\theta(\\tau)}$ is the expected cumulative reward of trajectory $\\tau = \\{a_1, o_1,..., a_n, o_n\\}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} E_{\\tau \\sim p_\\theta(\\tau)}[ \\sum_{t}^{\\infty} r(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "In our case, instead of assuming there is always a single special signal which represents the incentive, we believe that an intelligent system can drive itself to the maximization of a goal value which transformed from a sequence of input. \n",
    "\n",
    "Thus, we can construct a function $G(o)$ to. The goal function can be seen as the intention of a intelligent system. The identification of intelligence is equivalent to the search of the intention.\n",
    "\n",
    "\n",
    "**Textual Definition**: An intelligent system is a system which attempts to maximize its goal over time\n",
    "\n",
    "Because we don't know about the underlying structure, any structure of agent can serve us welll if it meets the requirement we set up. For example,\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_{t} + \\alpha \\frac{d G}{d \\theta} G(o_{t})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The W value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the effort of the agent given time $t$, we can construct a $W$ value as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "W_{\\{s_0,\\pi_\\theta,G\\}}(t) = E\\big(G(o_t)|a_{1}^{\\pi_\\theta},o_1...a_{t}^{\\pi_\\theta},o_{t}\\big).\n",
    "\\end{equation}\n",
    "\n",
    "$s_0$ - the inital state of the environment before the first observation $o_1$\n",
    "\n",
    "$\\pi$ - the observed agent defined by its policy $\\pi$\n",
    "\n",
    "Basically, the $W$ function tells us the expected $G$ value of an agent $\\pi$ can achieve from state s0 after time $t$. We use the policy $\\pi$ to identify agent $\\pi$ in order to keep our model independent from the underlying structure of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Zero-point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hardly discussed question in AI is - how we can tell something is not intelligent. The zero-point of intelligence is hardly discussed. How can we define its minimum if intelligence is a single-dimensioned metrics.\n",
    "\n",
    "The randomness of agent needs to be discussed when we know about the environment dynamics, but for now, but for now, let's assume we have no background knowledge of the environment and the agent at all and use a simple agent following random policy.\n",
    "\n",
    "Therefore, our baseline can be constructed as\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\pi_{R},G\\}}(t) = E\\big(G(o_t)|a_{1}^{\\pi_{R}},o_1...a_{t}^{\\pi_{R}},o_{t}\\big).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Definiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of intelligence can now be written in a mathemetical form. Since we only need to find an arbitary goal function. In other word, in order to tell if a system is intelligent or not, we just need to find one explanation of the behavior of the agent.\n",
    "\n",
    "**Theory**. For an observation starts at state $s_0$ and lasts for time $T$, a system $\\prod$ is **Perceviably Intelligent** if there exists a Goal function $G$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\pi,G\\}}(T) - W_{\\{s_0,\\pi^R,G\\}}(T) > \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $\\epsilon$ is the acceptance constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 n-scope intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deal with the problem from **2.1**, w  h\n",
    "\n",
    "Instead of a single step\n",
    "\n",
    "Do we really need this part????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For goal function of N steps\n",
    "\n",
    "\\begin{equation}\n",
    "G_{t,n}^{*} = \\sum_{t'=t}^{t+n} G(o_{t})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Intelligence through behavior interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2, we describe the minimum requirement for a system to be intelligent. But the more interesting problem we need to answer is how we can compare two separate systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 I-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if we follow the same logic from W-value, then it would be clear for us to define the comparable scalar of intelligence repect to the tuple $\\{s_0, G, R\\}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "I_{s_0, G, R} = \\frac {d\\, W_{s_0, G, R}}{d\\,t} \\approx \\frac{1}{t} \\big( W_{\\{s_0,\\pi,G\\}}(T) - W_{\\{s_0,\\pi^R,G\\}}(T) \\big)\n",
    "\\end{equation}\n",
    "\n",
    "We call this scalaer I-value. Apparently, there are some problems with this over-simplified model. We will try to address some of the concerns in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Exchange layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly speaking, we can outline the capability of a autonomous system as follow\n",
    "\n",
    "Capability = Sensitivity + Intelligence + Strength\n",
    "\n",
    "In order to address the above formula in an mathemetical way. We create a separate layer between the environment and the kernel of the agent, which named it exchange layer.\n",
    "\n",
    "1. $O$ - the space of raw observation\n",
    "2. $O^P$ - the space of perceviable obseravtion\n",
    "3. $A$ - the space of logic output\n",
    "4. $A^T$ - the space of actions which affect the environment\n",
    "\n",
    "We want to minimize the influence of **Sensitivity** and **Strength**, and only calculating the intelligence based on the logic part. \n",
    "\n",
    "\\begin{equation}\n",
    "o_i^p = P(o_i)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\prod,G\\}}(t) = E\\big(G(P_1(o_t))|a_{i1}^{},P_1(o_{i1})...a_{it}^{},P_1(o_{it})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{\\{s_0,\\prod,G\\}}(t) = E\\big(G(P_2(o_t))|a_{j1}^{},P_2(o_{j1})...a_{it}^{},P_2(o_{jt})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The division between logic layer and exchange layer is basically determined by the viewpoint of the observer. Different divisions can be applied to the same (agent, environment) combination when the observation point changed. A human can be viewed as , or a single logic layer with certain assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Balance Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two agent $\\pi_1, \\pi_2$ are considred to be balanced  \n",
    "\n",
    "Two intelligent system are **Balanced**\n",
    "\n",
    "Textual defintion: Two autonomous system are considered as **Balanced** if their **Sensitivity** and **Strength** are the equivilent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sensation Balance\n",
    "\n",
    "Given ${O_1, O_1^P, A_1, A_1^T, O_2, O_2^P, A_2, A_2^T}$, there exists a $G'$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall o \\in O_1^p, G(P_1(o)) = G'(P_2(o))\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Are we able to construct a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Strength Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Proof by reasoning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance of strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $o_i$ - observation\n",
    "2. $G(o_{t})$ - Goal function\n",
    "3. $G_{t}^{*}$ - n-scope goal function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend our definition of I to include more fancy staff\n",
    "\n",
    "Why intelligent system needs to be optimal?\n",
    "\n",
    "No, it does not need to be optimal at all. That's why intelligence can be different.\n",
    "\n",
    "Upperbound and lowerbound of intelligence??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different systems with the same problem setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intent inference????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal policy?\n",
    "important sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
