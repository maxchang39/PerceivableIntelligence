{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The goal of this notebook is to flatten the code from CS294-HW2, parameters will be hard coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "\n",
    "# the gym environment name\n",
    "#env_name = \"InvertedPendulum-v2\"\n",
    "#env_name = \"Breakout-ram-v0\"\n",
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "# number of hidden layers\n",
    "n_layers = 2\n",
    "\n",
    "# random seed\n",
    "seed_init = 0\n",
    "\n",
    "# number of interations\n",
    "n_iter = 100\n",
    "\n",
    "# maximum steps can be taken by the simulation\n",
    "max_path_length = None\n",
    "\n",
    "# dimension of the hidden layer\n",
    "size = 64\n",
    "\n",
    "# default LR\n",
    "learning_rate = 5e-3\n",
    "#learning_rate = 5e-1\n",
    "\n",
    "# number of iterations in one update of critic\n",
    "num_target_updates = 10\n",
    "\n",
    "# number of gradients per iteration \n",
    "num_grad_steps_per_target_update = 10\n",
    "\n",
    "# corresponds to 'render', whether to animate the trajactory or not\n",
    "animate = \"store_true\"\n",
    "\n",
    "# the minimum sum of total timestep for all trajectories in a single run\n",
    "min_timesteps_per_batch = 1000\n",
    "\n",
    "# discount over future reward, assume no discount for now\n",
    "gamma = 0.99\n",
    "\n",
    "# Whether the advantage should be normalized or not\n",
    "normalize_advantages = \"store_true\"\n",
    "\n",
    "# whether we use reward_to_go as baseline or not\n",
    "reward_to_go = \"store_true\"\n",
    "#reward_to_go = None\n",
    "\n",
    "# whether we like baseline or not\n",
    "nn_baseline = \"store_true\"\n",
    "\n",
    "# iterations between animation\n",
    "animation_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "seed = seed_init + 10*1\n",
    "\n",
    "# Set random seeds\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Is this env continuous, or self.discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, computation_graph_args, sample_trajectory_args, estimate_return_args):\n",
    "        super(Agent, self).__init__()\n",
    "        self.ob_dim = computation_graph_args['ob_dim']\n",
    "        self.ac_dim = computation_graph_args['ac_dim']\n",
    "        self.discrete = computation_graph_args['discrete']\n",
    "        self.size = computation_graph_args['size']\n",
    "        self.n_layers = computation_graph_args['n_layers']\n",
    "        self.learning_rate = computation_graph_args['learning_rate']\n",
    "\n",
    "        self.animate = sample_trajectory_args['animate']\n",
    "        self.max_path_length = sample_trajectory_args['max_path_length']\n",
    "        self.min_timesteps_per_batch = sample_trajectory_args['min_timesteps_per_batch']\n",
    "        self.animation_interval = sample_trajectory_args['animation_interval']\n",
    "\n",
    "        self.gamma = estimate_return_args['gamma']\n",
    "        self.reward_to_go = estimate_return_args['reward_to_go']\n",
    "        self.nn_baseline = estimate_return_args['nn_baseline']\n",
    "        self.normalize_advantages = estimate_return_args['normalize_advantages']\n",
    "        \n",
    "        self.timesteps_this_batch = 0\n",
    "\n",
    "    def init_tf_sess(self):\n",
    "        tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        self.sess.__enter__() # equivalent to `with self.sess:`\n",
    "        tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "    #========================================================================================#\n",
    "    #                           ----------PROBLEM 2----------\n",
    "    #========================================================================================#\n",
    "    def define_placeholders(self):\n",
    "        \"\"\"\n",
    "            Placeholders for batch observations / actions / advantages in policy gradient\n",
    "            loss function.\n",
    "            See Agent.build_computation_graph for notation\n",
    "\n",
    "            returns:\n",
    "                sy_ob_no: placeholder for observations\n",
    "                sy_ac_na: placeholder for actions - this is the policy pi\n",
    "                sy_adv_n: placeholder for advantages\n",
    "        \"\"\"\n",
    "        sy_ob_no = tf.placeholder(shape=[None, self.ob_dim], name=\"ob\", dtype=tf.float32)\n",
    "\n",
    "        if self.discrete:\n",
    "            # In case a single number is used to represent the action 1 or -1\n",
    "            sy_ac_na = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32)\n",
    "        else:\n",
    "            # Simultaneously actions at the same time\n",
    "            sy_ac_na = tf.placeholder(shape=[None, self.ac_dim], name=\"ac\", dtype=tf.float32)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Average advantage across all \n",
    "        sy_adv_n = tf.placeholder(shape=[None], name=\"adv\", dtype=tf.float32)\n",
    "        steps = tf.placeholder(shape=[None], name=\"steps\", dtype=tf.float32)\n",
    "\n",
    "        return sy_ob_no, sy_ac_na, sy_adv_n, steps\n",
    "\n",
    "\n",
    "    #========================================================================================#\n",
    "    #                           ----------PROBLEM 2----------\n",
    "    #========================================================================================#\n",
    "    def policy_forward_pass(self, sy_ob_no):\n",
    "        \"\"\" Constructs the symbolic operation for the policy network outputs,\n",
    "            which are the parameters of the policy distribution p(a|s)\n",
    "\n",
    "            arguments:\n",
    "                sy_ob_no: (batch_size, self.ob_dim)\n",
    "\n",
    "            returns:\n",
    "                the parameters of the policy.\n",
    "\n",
    "                if discrete, the parameters are the logits of a categorical distribution\n",
    "                    over the actions\n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "\n",
    "                if continuous, the parameters are a tuple (mean, log_std) of a Gaussian\n",
    "                    distribution over actions. log_std should just be a trainable\n",
    "                    variable, not a network output.\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "\n",
    "            Hint: use the 'build_mlp' function to output the logits (in the discrete case)\n",
    "                and the mean (in the continuous case).\n",
    "                Pass in self.n_layers for the 'n_layers' argument, and\n",
    "                pass in self.size for the 'size' argument.\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            # YOUR HW2 CODE HERE\n",
    "            sy_logits_na = build_mlp(sy_ob_no, self.ac_dim, \"policy_forward_pass\", self.n_layers, self.size)\n",
    "            return sy_logits_na\n",
    "        else:\n",
    "            # YOUR HW2 CODE HERE\n",
    "            sy_mean = build_mlp(sy_ob_no, self.ac_dim, \"policy_forward_pass\", self.n_layers, self.size)\n",
    "            sy_logstd = tf.get_variable(\"logstd\", shape=[self.ac_dim], trainable=True)\n",
    "            return (sy_mean, sy_logstd)\n",
    "\n",
    "    #========================================================================================#\n",
    "    #                           ----------PROBLEM 2----------\n",
    "    #========================================================================================#\n",
    "    def sample_action(self, policy_parameters):\n",
    "        \"\"\" Constructs a symbolic operation for stochastically sampling from the policy\n",
    "            distribution\n",
    "\n",
    "            arguments:\n",
    "                policy_parameters\n",
    "                    if discrete: logits of a categorical distribution over actions\n",
    "                        sy_logits_na: (batch_size, self.ac_dim)\n",
    "                    if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                        sy_mean: (batch_size, self.ac_dim)\n",
    "                        sy_logstd: (self.ac_dim,)\n",
    "\n",
    "            returns:\n",
    "                sy_sampled_ac:\n",
    "                    if discrete: (batch_size,)\n",
    "                    if continuous: (batch_size, self.ac_dim)\n",
    "\n",
    "            Hint: for the continuous case, use the reparameterization trick:\n",
    "                 The output from a Gaussian distribution with mean 'mu' and std 'sigma' is\n",
    "\n",
    "                      mu + sigma * z,         z ~ N(0, I)\n",
    "\n",
    "                 This reduces the problem to just sampling z. (Hint: use tf.random_normal!)\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            sy_logits_na = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            samples = tf.multinomial(logits=sy_logits_na, num_samples=1) # output of shape [batch_size, num_samples]\n",
    "            sy_sampled_ac = tf.reshape(samples, [-1]) # flatten to be of shape [batch_size]\n",
    "        else:\n",
    "            sy_mean, sy_logstd = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            z = tf.random_normal(tf.shape(sy_mean), mean=0.0, stddev=1.0)\n",
    "            sy_std = tf.exp(sy_logstd)\n",
    "            sy_sampled_ac = sy_mean + sy_std * z\n",
    "        return sy_sampled_ac\n",
    "\n",
    "    #========================================================================================#\n",
    "    #                           ----------PROBLEM 2----------\n",
    "    #========================================================================================#\n",
    "    def get_log_prob(self, policy_parameters, sy_ac_na):\n",
    "        \"\"\" Constructs a symbolic operation for computing the log probability of a set of actions\n",
    "            that were actually taken according to the policy\n",
    "\n",
    "            arguments:\n",
    "                policy_parameters\n",
    "                    if discrete: logits of a categorical distribution over actions\n",
    "                        sy_logits_na: (batch_size, self.ac_dim)\n",
    "                    if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                        sy_mean: (batch_size, self.ac_dim)\n",
    "                        sy_logstd: (self.ac_dim,)\n",
    "\n",
    "                sy_ac_na: \n",
    "                    if discrete: (batch_size,)\n",
    "                    if continuous: (batch_size, self.ac_dim)\n",
    "\n",
    "            returns:\n",
    "                sy_logprob_n: (batch_size)\n",
    "\n",
    "            Hint:\n",
    "                For the discrete case, use the log probability under a categorical distribution.\n",
    "                For the continuous case, use the log probability under a multivariate gaussian.\n",
    "        \"\"\"\n",
    "        if self.discrete:\n",
    "            sy_logits_na = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            # Softmax the raw policy_parameters to become probabilities which can sum up to 1\n",
    "            # Then times the action dimension\n",
    "            sy_logprob_n = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels=sy_ac_na, logits=sy_logits_na)\n",
    "        else:\n",
    "            sy_mean, sy_logstd = policy_parameters\n",
    "            # YOUR HW2 CODE_HERE\n",
    "            # calculate the z score of the sampled actions under the policy\n",
    "            sy_z = (sy_ac_na - sy_mean) / tf.exp(sy_logstd)\n",
    "            # this maximizes likelihood by pushing z towards 0 (mean of distribution)\n",
    "            sy_logprob_n = -0.5 * tf.reduce_mean(tf.square(sy_z), axis=1)\n",
    "        return sy_logprob_n\n",
    "\n",
    "    def build_computation_graph(self):\n",
    "        \"\"\"\n",
    "            Notes on notation:\n",
    "\n",
    "            Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "            that are computed later in the function\n",
    "\n",
    "            Prefixes and suffixes:\n",
    "            ob - observation\n",
    "            ac - action\n",
    "            _no - this tensor should have shape (batch self.size /n/, observation dim)\n",
    "            _na - this tensor should have shape (batch self.size /n/, action dim), this is the policy tensor\n",
    "            _n  - this tensor should have shape (batch self.size /n/)\n",
    "\n",
    "            Note: batch self.size /n/ is defined at runtime, and until then, the shape for that axis\n",
    "            is None\n",
    "\n",
    "            ----------------------------------------------------------------------------------\n",
    "            loss: a function of self.sy_logprob_n and self.sy_adv_n that we will differentiate\n",
    "                to get the policy gradient.\n",
    "        \"\"\"\n",
    "        self.sy_ob_no, self.sy_ac_na, self.sy_adv_n, self.steps = self.define_placeholders()\n",
    "\n",
    "        # The policy takes in an observation and produces a distribution over the action space\n",
    "        # sy_ob_no serves as a sequence of state, then result is a new policy (probability of action)\n",
    "        # based on this sequence of state\n",
    "        # each row in policy_parameters is a pi(at | st) with the st given by sy_ob_no\n",
    "        # \n",
    "        self.policy_parameters = self.policy_forward_pass(self.sy_ob_no)\n",
    "\n",
    "        # We can sample actions from this action distribution.\n",
    "        # This will be called in Agent.sample_trajectory() where we generate a rollout.\n",
    "        self.sy_sampled_ac = self.sample_action(self.policy_parameters)\n",
    "\n",
    "        # We can also compute the logprob of the actions that were actually taken by the policy\n",
    "        # This is used in the loss function.\n",
    "        \n",
    "        ### Comment ### \n",
    "        #### policy_parameters is the probability of action according to policy\n",
    "        #### self.sy_ac_na is the actual action label taken in the sampling\n",
    "        self.sy_logprob_n = self.get_log_prob(self.policy_parameters, self.sy_ac_na)\n",
    "\n",
    "        #========================================================================================#\n",
    "        #                           ----------PROBLEM 2----------\n",
    "        # Loss Function and Training Operation\n",
    "        #========================================================================================#\n",
    "        loss = None # YOUR CODE HERE\n",
    "        ## add a minus to make this problem a minimum problem\n",
    "        #loss = - tf.reduce_sum(self.sy_logprob_n) * tf.reduce_sum(self.sy_adv_n)\n",
    "        loss = - tf.reduce_sum(self.sy_logprob_n * self.sy_adv_n)\n",
    "        loss = tf.math.divide(loss, self.steps)\n",
    "        self.update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "\n",
    "        #========================================================================================#\n",
    "        #                           ----------PROBLEM 6----------\n",
    "        # Optional Baseline\n",
    "        #\n",
    "        # Define placeholders for targets, a loss function and an update op for fitting a\n",
    "        # neural network baseline. These will be used to fit the neural network baseline.\n",
    "        #========================================================================================#\n",
    "        if self.nn_baseline:\n",
    "            self.baseline_prediction = tf.squeeze(build_mlp(\n",
    "                                    self.sy_ob_no,\n",
    "                                    1,\n",
    "                                    \"nn_baseline\",\n",
    "                                    n_layers=self.n_layers,\n",
    "                                    size=self.size))\n",
    "            # YOUR_CODE_HERE\n",
    "            \n",
    "            self.sy_target_n = tf.placeholder(shape=[None], name=\"baseline_target\", dtype=tf.float32)\n",
    "            baseline_loss = tf.losses.mean_squared_error(self.sy_target_n, self.baseline_prediction)\n",
    "\n",
    "            self.baseline_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(baseline_loss)\n",
    "\n",
    "    def sample_trajectories(self, itr, env):\n",
    "        # Collect paths until we have enough timesteps\n",
    "        timesteps_this_batch = 0\n",
    "        paths = []\n",
    "        while True:\n",
    "            animate_this_episode=(len(paths)==0 and (itr % self.animation_interval == 0) and self.animate)\n",
    "            path = self.sample_trajectory(env, animate_this_episode)\n",
    "            paths.append(path)\n",
    "            timesteps_this_batch += pathlength(path)\n",
    "            if timesteps_this_batch > self.min_timesteps_per_batch:\n",
    "                self.timesteps_this_batch = timesteps_this_batch\n",
    "                break\n",
    "        return paths, timesteps_this_batch\n",
    "\n",
    "    def sample_trajectory(self, env, animate_this_episode):\n",
    "        ob = env.reset()\n",
    "        obs, acs, rewards = [], [], []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.1)\n",
    "            obs.append(ob)\n",
    "           \n",
    "            #====================================================================================#\n",
    "            #                           ----------PROBLEM 3----------\n",
    "            #====================================================================================#\n",
    "            \n",
    "            \n",
    "            # expand the observation to a 2-D array and sample the action based on it\n",
    "            ac = self.sess.run(self.sy_sampled_ac, \n",
    "                               feed_dict={self.sy_ob_no: np.expand_dims(ob, axis=0)}) # YOUR HW2 CODE HERE\n",
    "            ac = ac[0]\n",
    "            \n",
    "            acs.append(ac)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            rewards.append(rew)\n",
    "            steps += 1\n",
    "            if done or steps > self.max_path_length:\n",
    "                break\n",
    "        path = {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "                \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "        return path\n",
    "\n",
    "    #====================================================================================#\n",
    "    #                           ----------PROBLEM 3----------\n",
    "    #====================================================================================#\n",
    "    def sum_of_rewards(self, re_n):\n",
    "        \"\"\"\n",
    "            Monte Carlo estimation of the Q function.\n",
    "\n",
    "            let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "                Agent.sample_trajectories\n",
    "            let num_paths be the number of paths sampled from Agent.sample_trajectories\n",
    "\n",
    "            arguments:\n",
    "                re_n: length: num_paths. Each element in re_n is a numpy array\n",
    "                    containing the rewards for the particular path\n",
    "\n",
    "            returns:\n",
    "                q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
    "                    whose length is the sum of the lengths of the paths\n",
    "\n",
    "            ----------------------------------------------------------------------------------\n",
    "\n",
    "            Your code should construct numpy arrays for Q-values which will be used to compute\n",
    "            advantages (which will in turn be fed to the placeholder you defined in\n",
    "            Agent.define_placeholders).\n",
    "\n",
    "            Recall that the expression for the policy gradient PG is\n",
    "\n",
    "                  PG = E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * (Q_t - b_t )]\n",
    "\n",
    "            where\n",
    "\n",
    "                  tau=(s_0, a_0, ...) is a trajectory,\n",
    "                  Q_t is the Q-value at time t, Q^{pi}(s_t, a_t),\n",
    "                  and b_t is a baseline which may depend on s_t.\n",
    "\n",
    "            You will write code for two cases, controlled by the flag 'reward_to_go':\n",
    "\n",
    "              Case 1: trajectory-based PG\n",
    "\n",
    "                  (reward_to_go = False)\n",
    "\n",
    "                  Instead of Q^{pi}(s_t, a_t), we use the total discounted reward summed over\n",
    "                  entire trajectory (regardless of which time step the Q-value should be for).\n",
    "\n",
    "                      For this case, the policy gradient estimator is\n",
    "\n",
    "                          E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * Ret(tau)]\n",
    "\n",
    "                  where\n",
    "\n",
    "                      Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}.\n",
    "\n",
    "                  Thus, you should compute\n",
    "\n",
    "                      Q_t = Ret(tau)\n",
    "\n",
    "              Case 2: reward-to-go PG\n",
    "\n",
    "                  (reward_to_go = True)\n",
    "\n",
    "                  Here, you estimate Q^{pi}(s_t, a_t) by the discounted sum of rewards starting\n",
    "                  from time step t. Thus, you should compute\n",
    "\n",
    "                      Q_t = sum_{t'=t}^T gamma^(t'-t) * r_{t'}\n",
    "\n",
    "\n",
    "            Store the Q-values for all timesteps and all trajectories in a variable 'q_n',\n",
    "            like the 'ob_no' and 'ac_na' above.\n",
    "        \"\"\"\n",
    "        \n",
    "        q_n = []\n",
    "        if self.reward_to_go:\n",
    "            for ri in re_n:\n",
    "                disc = 1\n",
    "                sum = 0\n",
    "                q = []\n",
    "                for rj in reversed(ri):\n",
    "                    sum += rj*disc\n",
    "                    disc *= gamma\n",
    "                    q.insert(0,sum)\n",
    "                q_n += q\n",
    "        else:\n",
    "            for ri in re_n:\n",
    "                disc = 1\n",
    "                sum = 0\n",
    "                for rj in ri:\n",
    "                    sum += rj*disc\n",
    "                    disc *= gamma\n",
    "                    q_n.append(sum)\n",
    "        return np.array(q_n)\n",
    "\n",
    "    def compute_advantage(self, ob_no, q_n):\n",
    "        \"\"\"\n",
    "            Computes advantages by (possibly) subtracting a baseline from the estimated Q values\n",
    "\n",
    "            let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "                Agent.sample_trajectories\n",
    "            let num_paths be the number of paths sampled from Agent.sample_trajectories\n",
    "\n",
    "            arguments:\n",
    "                ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "                q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
    "                    whose length is the sum of the lengths of the paths\n",
    "\n",
    "            returns:\n",
    "                adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                    advantages whose length is the sum of the lengths of the paths\n",
    "        \"\"\"\n",
    "        #====================================================================================#\n",
    "        #                           ----------PROBLEM 6----------\n",
    "        # Computing Baselines\n",
    "        #====================================================================================#\n",
    "        if self.nn_baseline:\n",
    "            # If nn_baseline is True, use your neural network to predict reward-to-go\n",
    "            # at each timestep for each trajectory, and save the result in a variable 'b_n'\n",
    "            # like 'ob_no', 'ac_na', and 'q_n'.\n",
    "            #\n",
    "            # Hint #bl1: rescale the output from the nn_baseline to match the statistics\n",
    "            # (mean and std) of the current batch of Q-values. (Goes with Hint\n",
    "            # #bl2 in Agent.update_parameters.\n",
    "            \n",
    "            \n",
    "            #raise NotImplementedError\n",
    "            ### TODO\n",
    "            b_n = self.sess.run(self.baseline_prediction,\n",
    "                          feed_dict={self.sy_ob_no: ob_no})\n",
    "            b_n = (b_n - q_n.mean()) / q_n.std()\n",
    "            adv_n = q_n - b_n\n",
    "        else:\n",
    "            adv_n = q_n.copy()\n",
    "        return adv_n\n",
    "\n",
    "    def estimate_return(self, ob_no, re_n):\n",
    "        \"\"\"\n",
    "            Estimates the returns over a set of trajectories.\n",
    "\n",
    "            let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "                Agent.sample_trajectories\n",
    "            let num_paths be the number of paths sampled from Agent.sample_trajectories\n",
    "\n",
    "            arguments:\n",
    "                ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "                re_n: length: num_paths. Each element in re_n is a numpy array\n",
    "                    containing the rewards for the particular path\n",
    "\n",
    "            returns:\n",
    "                q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
    "                    whose length is the sum of the lengths of the paths\n",
    "                adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                    advantages whose length is the sum of the lengths of the paths\n",
    "        \"\"\"\n",
    "        q_n = self.sum_of_rewards(re_n)\n",
    "        adv_n = self.compute_advantage(ob_no, q_n)\n",
    "        #====================================================================================#\n",
    "        #                           ----------PROBLEM 3----------\n",
    "        # Advantage Normalization\n",
    "        #====================================================================================#\n",
    "        #if self.normalize_advantages:\n",
    "        # On the next line, implement a trick which is known empirically to reduce variance\n",
    "        # in policy gradient methods: normalize adv_n to have mean zero and std=1.\n",
    "        \n",
    "        if self.normalize_advantages:\n",
    "            #mean = sum(adv_n) / len(adv_n)\n",
    "            #std = (sum([(ai - mean)**2 for ai in adv_n]) / len(adv_n)) ** .5\n",
    "            adv_n = np.array(adv_n)\n",
    "            adv_n = (adv_n - adv_n.mean()) / adv_n.std() # YOUR HW2 CODE_HERE\n",
    "\n",
    "        return q_n, adv_n\n",
    "\n",
    "    def update_parameters(self, ob_no, ac_na, q_n, adv_n):\n",
    "        \"\"\"\n",
    "            Update the parameters of the policy and (possibly) the neural network baseline,\n",
    "            which is trained to approximate the value function.\n",
    "\n",
    "            arguments:\n",
    "                ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "                ac_na: shape: (sum_of_path_lengths).\n",
    "                q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
    "                    whose length is the sum of the lengths of the paths\n",
    "                adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                    advantages whose length is the sum of the lengths of the paths\n",
    "\n",
    "            returns:\n",
    "                nothing\n",
    "\n",
    "        \"\"\"\n",
    "        #====================================================================================#\n",
    "        #                           ----------PROBLEM 6----------\n",
    "        # Optimizing Neural Network Baseline\n",
    "        #====================================================================================#\n",
    "        if self.nn_baseline:\n",
    "            # If a neural network baseline is used, set up the targets and the inputs for the\n",
    "            # baseline.\n",
    "            #\n",
    "            # Fit it to the current batch in order to use for the next iteration. Use the\n",
    "            # baseline_update_op you defined earlier.\n",
    "            #\n",
    "            # Hint #bl2: Instead of trying to target raw Q-values directly, rescale the\n",
    "            # targets to have mean zero and std=1. (Goes with Hint #bl1 in\n",
    "            # Agent.compute_advantage.)\n",
    "\n",
    "            # YOUR_CODE_HERE\n",
    "            # TODO: Why q_n is target\n",
    "            target_n = (q_n - q_n.mean()) / q_n.std() # YOUR HW2 CODE_HERE\n",
    "            self.sess.run(self.baseline_update_op,\n",
    "                      feed_dict={self.sy_ob_no: ob_no, self.sy_target_n: target_n})\n",
    "\n",
    "\n",
    "        #====================================================================================#\n",
    "        #                           ----------PROBLEM 3----------\n",
    "        # Performing the Policy Update\n",
    "        #====================================================================================#\n",
    "\n",
    "        # Call the update operation necessary to perform the policy gradient update based on\n",
    "        # the currenat batch of rollouts.\n",
    "        #\n",
    "        # For debug purposes, you may wish to save the value of the loss function before\n",
    "        # and after an update, and then log them below.\n",
    "\n",
    "        # YOUR_CODE_HERE\n",
    "        \n",
    "        ####Comment####\n",
    "        # run the Adam optimizer we defined above with everything updated\n",
    "        self.sess.run(self.update_op,\n",
    "                      feed_dict={self.sy_ob_no: ob_no, self.sy_ac_na: ac_na, self.sy_adv_n: adv_n, self.steps : np.array([1000])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_placeholder, output_size, scope, n_layers, size, activation=tf.tanh, output_activation=None):\n",
    "    \"\"\"\n",
    "        Builds a feedforward neural network\n",
    "\n",
    "        arguments:\n",
    "            input_placeholder: placeholder variable for the state (batch_size, input_size)\n",
    "            output_size: size of the output layer\n",
    "            scope: variable scope of the network\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of the hidden layer\n",
    "            activation: activation of the hidden layers\n",
    "            output_activation: activation of the ouput layers\n",
    "\n",
    "        returns:\n",
    "            output placeholder of the network (the result of a forward pass)\n",
    "\n",
    "        Hint: use tf.layers.dense\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # with variable scope _scope\n",
    "    batch_size = input_placeholder[1]\n",
    "    input_size = input_placeholder[2]\n",
    "\n",
    "    # network = tf.keras.models.Sequential([\n",
    "    # tf.keras.layers.Flatten(input_shape=input_size),\n",
    "    # tf.keras.layers.Dense(size, activation=activation),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "    # tf.keras.layers.Dense(output_size, activation=output_activation)\n",
    "    # ])\n",
    "    #input_layer = tf.reshape(input_placeholder, [-1, 28, 28, 1])\n",
    "    input_layer = input_placeholder\n",
    "    hidden_input = input_layer\n",
    "\n",
    "    for i in range(0, n_layers):\n",
    "        hidden_input = tf.layers.dense(inputs=hidden_input, units=size, activation=activation)\n",
    "\n",
    "    output_placeholder = tf.layers.dense(inputs=hidden_input, units=output_size, activation=output_activation)\n",
    "\n",
    "    return output_placeholder\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path[\"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 23:24:11.038002 140735683527552 deprecation.py:323] From <ipython-input-5-afc69693cd93>:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0722 23:24:11.043020 140735683527552 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0722 23:24:11.561825 140735683527552 deprecation.py:323] From <ipython-input-4-13851732996c>:128: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "W0722 23:24:11.875457 140735683527552 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "computation_graph_args = {\n",
    "    'n_layers': n_layers,\n",
    "    'ob_dim': ob_dim,\n",
    "    'ac_dim': ac_dim,\n",
    "    'discrete': discrete,\n",
    "    'size': size,\n",
    "    'learning_rate': learning_rate\n",
    "}\n",
    "\n",
    "sample_trajectory_args = {\n",
    "    'animate': animate,\n",
    "    'max_path_length': max_path_length,\n",
    "    'min_timesteps_per_batch': min_timesteps_per_batch,\n",
    "    'animation_interval' : animation_interval\n",
    "}\n",
    "\n",
    "estimate_return_args = {\n",
    "    'gamma': gamma,\n",
    "    'reward_to_go': reward_to_go,\n",
    "    'nn_baseline': nn_baseline,\n",
    "    'normalize_advantages': normalize_advantages,\n",
    "}\n",
    "\n",
    "agent = Agent(computation_graph_args, sample_trajectory_args, estimate_return_args)\n",
    "\n",
    "# build computation graph\n",
    "agent.build_computation_graph()\n",
    "\n",
    "# tensorflow: config, session, variable initialization\n",
    "agent.init_tf_sess()\n",
    "\n",
    "#========================================================================================#\n",
    "# Training Loop\n",
    "#========================================================================================#\n",
    "\n",
    "total_timesteps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n",
      "timesteps:  1004\n",
      "returns: 27.88889\n",
      "********** Iteration 1 ************\n",
      "timesteps:  1026\n",
      "returns: 35.37931\n",
      "********** Iteration 2 ************\n",
      "timesteps:  1008\n",
      "returns: 37.333332\n",
      "********** Iteration 3 ************\n",
      "timesteps:  1028\n",
      "returns: 54.105263\n",
      "********** Iteration 4 ************\n",
      "timesteps:  1017\n",
      "returns: 63.5625\n",
      "********** Iteration 5 ************\n",
      "timesteps:  1046\n",
      "returns: 69.73333\n",
      "********** Iteration 6 ************\n",
      "timesteps:  1033\n",
      "returns: 64.5625\n",
      "********** Iteration 7 ************\n",
      "timesteps:  1011\n",
      "returns: 67.4\n",
      "********** Iteration 8 ************\n",
      "timesteps:  1046\n",
      "returns: 74.71429\n",
      "********** Iteration 9 ************\n",
      "timesteps:  1041\n",
      "returns: 74.35714\n",
      "********** Iteration 10 ************\n",
      "timesteps:  1079\n",
      "returns: 77.07143\n",
      "********** Iteration 11 ************\n",
      "timesteps:  1007\n",
      "returns: 100.7\n",
      "********** Iteration 12 ************\n",
      "timesteps:  1043\n",
      "returns: 173.83333\n",
      "********** Iteration 13 ************\n",
      "timesteps:  1109\n",
      "returns: 221.8\n",
      "********** Iteration 14 ************\n",
      "timesteps:  1342\n",
      "returns: 335.5\n",
      "********** Iteration 15 ************\n",
      "timesteps:  1229\n",
      "returns: 204.83333\n",
      "********** Iteration 16 ************\n",
      "timesteps:  1191\n",
      "returns: 297.75\n",
      "********** Iteration 17 ************\n",
      "timesteps:  1321\n",
      "returns: 330.25\n",
      "********** Iteration 18 ************\n",
      "timesteps:  1127\n",
      "returns: 375.66666\n",
      "********** Iteration 19 ************\n",
      "timesteps:  1020\n",
      "returns: 340.0\n",
      "********** Iteration 20 ************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c5a813eabaf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"********** Iteration %i ************\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtimesteps_this_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-13851732996c>\u001b[0m in \u001b[0;36msample_trajectories\u001b[0;34m(self, itr, env)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0manimate_this_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimation_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimate_this_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpathlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-13851732996c>\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(self, env, animate_this_episode)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manimate_this_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "\n",
    "for itr in range(n_iter):\n",
    "    print(\"********** Iteration %i ************\"%itr)\n",
    "    paths, timesteps_this_batch = agent.sample_trajectories(itr, env)   \n",
    "    total_timesteps += timesteps_this_batch\n",
    "\n",
    "    # Build arrays for observation, action for the policy gradient update by concatenating\n",
    "    # across paths\n",
    "    ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "    ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "\n",
    "    q_n, adv_n = agent.estimate_return(ob_no, re_n)\n",
    "\n",
    "    agent.update_parameters(ob_no, ac_na, q_n, adv_n)\n",
    "    \n",
    "    returns = [path[\"reward\"].sum() for path in paths]\n",
    "    \n",
    "    print(\"timesteps: \", agent.timesteps_this_batch);    \n",
    "    print(\"returns:\", np.mean(returns))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax:\n",
    "\n",
    "**Softmax** $(x_{i}) = e^{x_i}  \\sum_{j}e^{x_j}$\n",
    "\n",
    "Convert logits into probabilities which can sum up to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test softmax\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x = [ 0.8360188,   0.11314284,  0.05083836]\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CrossEntropy** $(x) = -\\sum_{i} p(x_i) \\log q(x_i)$\n",
    "\n",
    "maximizing the likelihood of training set is the same as minimizing the cross entropy.\n",
    "\n",
    "p(x) in cross-entropy equation is true distribution, while q(x) is the distribution obtained from softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1 = [1,2,3,4]\n",
    "path1 = {\"reward\" : np.array(reward1, dtype=np.float32)}\n",
    "reward2 = [3,4,5,6]\n",
    "path2 = {\"reward\" : np.array(reward2, dtype=np.float32)}\n",
    "paths = [path1, path2]\n",
    "r = [path[\"reward\"] for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.900499, 8.9302, 6.97, 4.0, 17.781297, 14.8704, 10.95, 6.0]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_n = []\n",
    "gamma = 0.99\n",
    "for ri in r:\n",
    "    disc = 1\n",
    "    sum = 0\n",
    "    q = []\n",
    "    for rj in reversed(ri):\n",
    "        sum += rj*disc\n",
    "        disc *= gamma\n",
    "        #q_n.append(sum)\n",
    "        q.insert(0,sum)\n",
    "    q_n += q\n",
    "    \n",
    "#     for rj in ri:\n",
    "\n",
    "        \n",
    "q_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "a = [1,2,3,4,5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
